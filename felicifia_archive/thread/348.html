<!doctype html>
<html>
    <head>
        <meta charset="utf-8">
        <title>Felicifia: global utilitarian discussion &bull; View topic - Existential risk reduction cost effectiveness</title>
        <script type="text/javascript" src="../styles/nexus/template/styleswitcher.js"></script>
        <script type="text/javascript" src="../styles/nexus/template/forum_fn.js"></script>
        <script type="text/javascript" src="../styles/custom.js"></script>
        <link href="../styles/nexus/theme/print.css" rel="stylesheet" type="text/css" media="print" title="printonly"/>
        <link href="../styles/prosilver.css" rel="stylesheet" type="text/css" media="screen, projection"/>
        <link href="../styles/nexus/theme/normal.css" rel="stylesheet" type="text/css" title="A"/>
        <link href="../styles/nexus/theme/medium.css" rel="alternate stylesheet" type="text/css" title="A+"/>
        <link href="../styles/nexus/theme/large.css" rel="alternate stylesheet" type="text/css" title="A++"/>
        <link href="../styles/custom.css" rel="stylesheet" type="text/css"/>
        <script type="text/javascript"><!--
            var spoiler_show = "[Reveal]";
            var spoiler_hide = "[Obscure]";
            //-->
        </script>
        <script type="text/javascript" src="../styles/nexus/template/prime_bbcode_spoiler.js"></script>
        <link href="../styles/nexus/theme/prime_bbcode_spoiler.css" rel="stylesheet" type="text/css"/>
    </head>
    <body id="phpbb" class="section-viewtopic ltr">
        <div id="mainframe">
        <div class="top-left"></div>
        <div class="top-middle"></div>
        <div class="top-right"></div>
        <div class="inner-wrap">
            <div class="positioncorrection-top">
                <div id="wrap">
                    <a id="top" name="top" accesskey="t"></a>
                    <div id="page-header">
                        <div class="headerbar">
                            <div class="inner">
                                <span class="corners-top"><span></span></span>
                                <div id="site-description">
                                    <a href="../forum/index.html" title="Board index" id="logo"><img src="../styles/nexus/imageset/simple%20logo.png" alt="" title="" width="766" height="126"></a>
                                    <p style="display: none;"><a href="#start_here">Skip to content</a></p>
                                </div>
                                <span class="corners-bottom"><span></span></span>
                            </div>
                        </div>
                        <div class="navbar">
                            <div class="inner">
                                <span class="corners-top"><span></span></span>
                                <ul class="linklist navlinks">
                                    <li class="icon-home"><a href="../forum/index.html" accesskey="h">Board index</a>  <strong>‹</strong> <a href="../forum/23.html">Applied ethics</a></li>
                                    <li class="rightside"><a href="#" onclick="fontsizeup(); return false;" onkeypress="fontsizeup(); return false;" class="fontsize" title="Change font size">Change font size</a></li>
                                </ul>
                                <span class="corners-bottom"><span></span></span>
                            </div>
                        </div>
                    </div>
                    <!--
                        <div class="google">

                        </div>
                        -->
                    <a name="start_here"></a>
                    <div id="page-body">
                        <h2><a href="#">Existential risk reduction cost effectiveness</a></h2>
                        <!-- NOTE: remove the style="display: none" when you want to have the forum description on the topic body --><span style="display: none">Whether it's pushpin, poetry or neither, you can discuss it here.<br></span>
                        <div class="topic-actions">
                            <div class="buttons">
                            </div>
                            <div class="pagination">
                                13 posts
                            </div>
                        </div>
                        <div class="clear"></div>
                        <div class="post bg2" id="p2450">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2450">Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/169.html">Jesper Östman</a></strong> on 2010-12-25T18:02:00</p>
<div class="content"><div class="postrev" data-snap="0">My plan is to do a quick fermi-calculation for the minimum(note1) expected happy(note2) life years per dollar (hly/$) we can gain by reducing existential risk. Such a number will help us decide whether to use our resources for reducing existential risk or other purposes, like reducing meat consumption or helping poor humans. I will do this by using results from two superb papers, Nick Bostrom's  <a class="postlink" href="http://www.nickbostrom.com/astronomical/waste.pdf">"Astronomical Waste"</a> and Gaverick Matheny's  <a class="postlink" href="http://jgmatheny.org/matheny_extinction_risk.htm">"Reducing the Risk of Human Extinction"</a> and a couple of other assumptions. Matheny calculates the exptected hly/$ gain assuming humans remain on earth. Bostrom considers the implications of space-colonization. I will modify Matheny's result for space colonization considerations (c-adjusted).<br/><br/>Matheny's result, without time-discounting, is that an asteroid screening program will give us 0.4 hly/$. The source of the hly are the expected lives of all the future human generations which will live on earth if an asteroid does not make humans go extinct (within the next 100 years, he assumes that after that point we will be able to handle asteroids in any case??). Since he assumes that the human population remains on earth, the number of expected future humans will be astronomically larger if we assume a succesful human space colonization. I will make a rough estimate of this number multiplying Matheny's hly/$ result with the ratio of the  c-adjusted total hly number and Matheny's hly number. <br/><br/>We get the total number of human hly by multiplying the population size (hly/y) with its lifetime (y). Matheny assumes a population of 10^10 which lasts for 1.6*10^6 years. That totals 1.6*10^16 hly. How much larger is the c-adjusted number? Bostrom considers the utility of a colonization of our local supercluster Virgo. According to him the cluster can support 10^23 humans, assuming just a conservative 10^10 humans on average around each star (note3). This is my c-adjusted hly/y estimate. Assuming we can get this energy output for 10^11 years(note4) our total is 10^34 hly. That gives us a ratio between the c-adjusted result and Matheny's hly of about 10^18. <br/><br/>Matheny's non-discounted hly/$ number was 0.4 So the c-adjusted hly/$ will be 4*10^17. Assuming the probability of a happy supercluster colonization is only 0.01 we still get the result that the c-adjusted number is an astronomical 4*10^15 hly/$. This is a huge number. It says that each dollar in the asteroid screening program will net us an expected four million billions of happy life years.<br/><br/>For comparison, estimated hly/$ for top non-existential-risk utilitarian interventions:<br/><br/>Vegan Outreach: 0.55-25 hly/$  (estimate by Alan Dawrst <a class="postlink" href="http://www.utilitarian-essays.com/dollar-worth.pdf">"How much is a dollar worth, the case of vegan outreach"</a>)<br/>VillageReach vaccination: 0.15 hly/$  (saves a life for 545$, although it's unlikely I assume that one saved life gives 80 hly, so we get some 0.15 hly/$)<br/><br/>My conclusion is that even if an (acceptable by utilitarian standards) supercluster colonization is a lot less likely than 1/100 for a utilitarian asteroid screening and other long-term existential risk interventions will give far more expected utility than even the most effective short-term interventions. <br/><br/>Some comments:<br/>a) Note that nothing more speculative than technology for self-replicating space-colonization and some hedonic enhancement is needed for this result (eg no singularity, superintelligence, molecular nanotechnology or uploading). <br/><br/>b) In particular, note that with access to conscious ai or brain emulation and powerful energy harvesting technology we could get up to 17 orders of magnitude more hly/$ (see note 3).<br/><br/>c) The 1/100 likelihood assumption includes the possibilities that we can't get space colonization technology, that we become extinct for some other reason, or that we wouldn't be motivated to colonize that much. Subjectively, I think the biggest obstacle here is the other risks. However, two considerations may make it reasonable not to give survival estimates several orders of magnitude less than this: (1) many experts in the global catastrophic risk subject believe that the era before we start colonizing space will be especially dangerous (2) all of these experts give survival estimates above 50%. (note5)<br/><br/>d) Asteroid screening may not be the most effective way to reduce existential risk. Compare, for example the utility of building a self-sustaining bunker. According to Matheny its cost would be in the same order of magnitude as the asteroid program. The latter would reduce a 1 in a billion risk by 50%. Subjectively, it seems like a conservative estimate is that a bunker could reduce a total risk of say at least 10% from biotech and other technologies to at least 9.5%. If that's the case than we would get some 7 orders of magnitude more utility. In this case, if we also use Bostrom's more liberal estimate we could get up to 4*10^39 hly/$ from a bunker project.<br/><br/><br/>Note 1<br/>Minimum because it is based on the numbers for an asteroid screening program, there may be other ways of reducing existential risk which are even more effective.<br/><br/>Note 2<br/>I assume that future hedonic enhancement technology will ensure that the overwhelming majority of all these future life years will be "happy" from a hedonistic utilitarian perspective.<br/><br/>Note 3<br/><br/>Bostrom also considers a more ambitious scenario. In this scenario we use advanced molecular nano-technology to harness the total computing power from each star and use it to run as many human minds as possible. In this case we would get 17 orders of magnitude more hly.<br/><br/>Note 4<br/><br/>100*10^9 years =10^11 years, assuming the energy output will be roughly constant for the "current era of star formation".<br/><br/>"The current era of star formation is expected to continue for up to one hundred billion years, and then the "stellar age" will wind down after about ten trillion to one hundred trillion years (1013-1014 years), as the smallest, longest-lived stars in our astrosphere, tiny red dwarfs, begin to fade. At the end of the stellar age, galaxies will be composed of compact objects: brown dwarfs, white dwarfs that are cooling or cold ("black dwarfs"), neutron stars, and black holes. Eventually, as a result of gravitational relaxation, all stars will either fall into central supermassive black holes or be flung into intergalactic space as a result of collisions.[95][96]"<br/>from <a class="postlink" href="http://en.wikipedia.org/wiki/Galaxy#Formation_and_evolution">http://en.wikipedia.org/wiki/Galaxy#Formation_and_evolution</a><br/><br/>Note 5<br/><br/>These are some of the main sources on existential risk estimates, note that all are far below 100% risk. Of course, if the risk post space colonization hasn't decreased enough cumulative risk over long periods of time could get very high. But it seems that with an exponentially increasing expansion most risks would quickly decrease.<br/><br/>Estimated probability of extinction (or similar):<br/>50% (of a disastrous setback of civilization) in the next 100 years according to Sir Martin Reese <a class="postlink" href="http://www.amazon.com/Our-Final-Hour-Scientists-Warning/dp/0465068634">Our final century/hour (2004)</a> <br/>30% in the next 500 years according to John Leslie - <a class="postlink" href="http://www.amazon.com/End-World-Science-Ethics-Extinction/dp/0415140439">The End of the World (1996)</a> <br/>"Significant risk" according to Richard Posener - <a class="postlink" href="http://www.amazon.com/Catastrophe-Risk-Response-Richard-Posner/dp/0195178130">Catastrophe: Risk and Response (2005)</a><br/>&gt;25% Bostrom,  <a class="postlink" href="http://www.nickbostrom.com/existential/risks.html">"Existential Risks" (2002)</a></div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2450">
<dt>
<a href="../user/169.html"></a><br/>
<a href="../user/169.html">Jesper Östman</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 159</dd>
<dd><strong>Joined:</strong> Mon Oct 26, 2009 5:23 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2451">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2451">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2010-12-25T21:31:00</p>
<div class="content"><div class="postrev" data-snap="0">First off, is the doomsday argument taken into account? Also, EDT could lessen the effect, depending on how your priors work. We don't really know how dangerous the universe is, and if the prior for how dangerous the universe is isn't independent from the priors for how long humanity would live given an amount of danger, this means making the universe safer will mean that the universe was more dangerous in the first place.<br/><br/>Considering that if the prior for total number of people doesn't fall off quickly enough expected utility will diverge, but this problem doesn't apply to anything else, it seems somewhat reasonable to make the the priors weird like that.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile2451">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2453">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2453">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/169.html">Jesper Östman</a></strong> on 2010-12-25T23:12:00</p>
<div class="content"><div class="postrev" data-snap="0">Good point. No, I haven't included <a class="postlink" href="http://en.wikipedia.org/wiki/Doomsday_argument">Doomsday</a>, <a class="postlink" href="http://en.wikipedia.org/wiki/Great_Filter">Filter</a>, or <a class="postlink" href="http://www.simulation-argument.com/">Simulation</a> considerations. The same goes for these particular papers from Bostrom and Matheny. Perhaps it can be argued that these arguments ensure that it is virtually impossible for humanity to survive long enough for a substantial space colonization. Of course, these arguments are also controversial.</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2453">
<dt>
<a href="../user/169.html"></a><br/>
<a href="../user/169.html">Jesper Östman</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 159</dd>
<dd><strong>Joined:</strong> Mon Oct 26, 2009 5:23 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2455">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2455">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/341.html">rehoot</a></strong> on 2010-12-26T00:44:00</p>
<div class="content"><div class="postrev" data-snap="0">Maybe rough calculations would help to put alternatives into perspective. <br/><br/>Things to consider:<br/><br/>1) Adjust the cost of the plan by sending robots (instead of humans). They would carry some frozen germs or embryos for future reanimation--or maybe give the robots the knowledge to synthesis life from inanimate chemicals that can either be shipped or discovered at the destination (that reduces the risk that radiation would mutate everything in transit). Robots would build the technological infrastructure as the human population grows (calculated the risk that the robots would turn the humans into slaves).  The robots would run on Mac OS X or Ubuntu, not Windows!!!<br/><br/>2) Consider social interaction effects.  For example, reallocation of money to a project to save a few White families (capitalist families, wealthy families...) leads to civil unrest, then anarchy, then self inihilation.<br/><br/>3) Consider alternatives to travel expences (e.g., use SETI to call for a ride).<br/><br/>4) After having committed to the recolonization plan, an asteroid approaches, and is a few years away with immenant collision. Finger-pointing commences.  The recolonization plan is scrapped, but the asteroid either misses or does not kill everybody, but the recolonization plan is dead with the initial investment serving only as an amusement park.<br/><br/>5) What is the cost relative to alternative plans for survival in this solar system (e.g., the bunker you mentioned, living under the sea, living on Mars...).<br/><br/>6) What would be the cost of sending a self-perpetuating contingent of humans that could maintain the technology needed to continue planet-hoping?  That expense might raise total cost exponentially.<br/><br/>7) Does a planet with 1 x 10^10 people really imply that they are happy? <br/><br/>8) What is the basis for the 1.6*10^6 years of human existence on earth? There might be great variation based on that number. Would the numbers change if some survivers lived and devolved into apes or an intermediate between humans and apes?<br/><br/>9) Consider unforeseen effects of relocating (radiation, traveling for four generations only to be inihilted by a tiny piece of space junk).  This might demand a multiplicity of voyages and thereby increase costs.</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2455">
<dt>
<a href="../user/341.html"></a><br/>
<a href="../user/341.html">rehoot</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 161</dd>
<dd><strong>Joined:</strong> Wed Dec 15, 2010 7:32 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2457">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2457">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/53.html">RyanCarey</a></strong> on 2010-12-26T08:57:00</p>
<div class="content"><div class="postrev" data-snap="0">As far as I can understand, you've argued that preserving the life on Earth is far more important that it might appear, because we can colonise other planets. But that only addresses one of the two key questions here. The other key question is whether humans (presently, and in the future) will contribute or detract from the amount of happiness in the universe.</div><div class="diff hidden"></div></div>
<div class="signature">You can read my personal blog here: <a class="postlink" href="http://www.careyryan.com">CareyRyan.com</a></div>
</div>
<dl class="postprofile" id="profile2457">
<dt>
<a href="../user/53.html"><img alt="User avatar" height="100" src="../file/53_1225370811.gif" width="100"/></a><br/>
<a href="../user/53.html">RyanCarey</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 682</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 1:01 am</dd>
<dd><strong>Location:</strong> Melbourne, Australia</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2464">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2464">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/169.html">Jesper Östman</a></strong> on 2010-12-26T16:59:00</p>
<div class="content"><div class="postrev" data-snap="0">Good point Ryan, I'll reply in two parts.<br/><br/>Can we assume that the lives of space colonizing humans would on average make a positive contribution? I'll split this question into two subquestions: (I) Can we expect future human lives to be happy on average? (II) Would they create enough (animal) suffering to outweigh their own happiness? <br/><br/><span style="font-weight: bold">(I): The happiness assumption</span> <br/><br/>(1). Personally, I think it is very likely that future human lives will be above 0, or even far above 0. So to me (2) seems to be the important concern here. Let me explain. My reason is that use of hedonic enhancement technology will likely be widespread. This is because such technology will be (a) available, (b) cheap and (c) in great demand.<br/><br/>(a) It seems unlikely that a civilisation with the capability for star-faring would not be capable of <a class="postlink" href="http://www.hedweb.com/confile.htm">hedonic enhancement</a>. In particular it seems hard to deny our space colonizing future humans such technology since the time-scales we are considering are thousands, millions or even billions of years and the resources available to them will be immense (perhaps somewhere between II and III on the <a class="postlink" href="http://en.wikipedia.org/wiki/Kardashev_scale">Kardashev scale</a>). For example, since most of our current western long-term happiness variation is <a class="postlink" href="http://www.biomedsearch.com/nih/Subjective-well-being-genetic-environmental/16749947.html">determined by our genes</a> specially designed drugs or gene therapy should be able to make average future humans much happier than contemporary humans. At the very least gene therapy should allow the average future human to have life about as good as happiest contemporary humans. So if any human in history had a life worth living, then the combined happiness of a space colonizing civilisation using such technology will amount to an astronomical positive number.  An uploaded non-biological posthuman civilisation could of course enhance its hedonic states even easier through mere software manipulation.<br/><br/>Furthermore, it is likely the far future humans we are considering could achieve mental states far happier and more pleasant, perhaps by several orders of magnitude, than even the happiest moments in contemporary human lives. If that is likely the case the cost effectiveness of existentialrisk reduction would increase by several orders of magnitude again.<br/><br/>(b) Would the technology be availabe to most people? Manipulating our brains (or computer software) would neither require huge amounts of energy nor natural resources, so we can expect that basic hedonic enhancement technology will eventually become cheap and readily available. This seems especially likely when we consider timeframes of several millions of years of technological development. <br/><br/>(c) Since people are prepared to spend large amounts of money on products which promise to increase happiness in different ways but are very ineffective (eg much of our material consumption) at doing this or have big downsides (eg many contemporary recreational drugs) it seems clear almost all people would strongly prefer to use cheap, reliable hedonicenhancement technology when available. This should especially be the case when the use of such technology has thousands of years available for becoming a part of the culture.<br/><br/>My conclusion on (I) is that we have good reason to believe supercluster colonizing humans (posthumans) will be happy.<br/><br/>(to be continued)</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2464">
<dt>
<a href="../user/169.html"></a><br/>
<a href="../user/169.html">Jesper Östman</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 159</dd>
<dd><strong>Joined:</strong> Mon Oct 26, 2009 5:23 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2467">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2467">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/169.html">Jesper Östman</a></strong> on 2010-12-26T22:21:00</p>
<div class="content"><div class="postrev" data-snap="0"><span style="font-weight: bold">Rehoot:</span><br/><br/>Thanks for the comments! Note that what I'm assuming is that humans will likely colonize space if we don't die out during a transitional period. So the cost-effectiveness calculations are for the interventions I'm considering (asteroid screening program, building a self-sustaining bunker) and not space exploration, since that's assumed to happen anyway as long as we survive (of course, trying to get an earlier/quicker/cheaper space colonization would also be a strategy for avoiding existential risk).<br/><br/>I take it that 1) and 3), 4) 6) concern optimal space colonization and not the other projects for reducing existential risk. In particular I agree that digital life-forms seem more suitable for star faring.<br/><br/>2) I take it this point is about the bunker suggestion. Yes, that would be one thing to consider. But I'm not sure how likely such unrest would be as long as there is no catastrophe in sight. <br/><br/>7) See post above.<br/>8) That number might be more precise for apes than for humans actually. It is Matheny's estimate, based on the life-span of our closest relative, homo erectus.<br/>9)  Incidentally, not that dangerous</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2467">
<dt>
<a href="../user/169.html"></a><br/>
<a href="../user/169.html">Jesper Östman</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 159</dd>
<dd><strong>Joined:</strong> Mon Oct 26, 2009 5:23 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2507">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2507">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2011-01-04T21:01:00</p>
<div class="content"><div class="postrev" data-snap="0">I have a couple of reservations (in addition to a general scepticism about Bostrom's premises):<br/><br/>1) It's not obvious to me how 'short-term' short-term happiness is, so it's not obvious we're comparing like with like. Existential risk reduction looks good because we assume its effects are felt a long way off. <br/><br/>Immediate happiness gets it in the neck because we don't assume hedons are powerful enough self-replicators to affect the indefinite future. But if you can find hedon-generating cause whose hedons are expected to replicate &gt;1 times, you have near-infinite expected future hedons from just generating one now - a fair bit worse than 1:1 replication can still allow very high expectation, IIRC, using similar maths to Bostrom's - you don't need high probability of future survival/replication to give the hedons room to expand in expectation-space (if that's a useful concept), so long as you have a sufficiently high potential number.<br/><br/>A simple example is just being pleasant to someone. It's pretty clear that people who've recently had a pleasant interaction are - in at least some cases - more likely to interact pleasantly. It's not clear how much, obviously. And presumably there will be more effective hedon-replicating memes than simple pleasantries.<br/><br/>2) Comparing existential risk reductions with each other is incredibly difficult. Some, like asteroid defence are <span style="font-style: italic">relatively</span> concrete, but others, like reducing the risk of nuclear/biological war (which seem - without much justification that I can offer - like much bigger threats to me than a species-killing impact, whose chances in the next century I found from a quick search a <a class="postlink" href="http://abcnews.go.com/Technology/story?id=98208&amp;page=1">1/5000 guesstimate</a> for - but it's not clear whether they're restricting the possibilities to an impact that would actually wipe out humanity), depend on the unmodelable interaction of societies and certain key individuals.<br/><br/>If one assumes that human-animosity-caused extinction events are as or more likely than any others, then a combination of 1) and 2) make me think that 'conventional' causes look much better than futurist thinkers tend to view them. Partly because happiness propagation is potentially massively better than we normally see it, and partly because effective happiness propagation itself, using the reasoning in 1), seems like it must have an impact on reducing the quantity of animosity and thus the likelihood that animosity will destroy the world.<br/><br/>On this view, Toby's proposed charities look stronger than ever to me (although I can imagine complications with human psychology, eg people being more grateful for cure than prevention, even though the latter tends to be much more cost-effective in its own right), though animal welfare charities seem likely to suffer - animals are in a very weak position to spread memes. Promoting utilitarianism seems like a potential winner, too. I also feel that attention to climate change is potentially high on the list, since it's a political issue that economically threatens many of the more powerful players (and thus threatens to increase their animosity).</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile2507">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2580">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2580">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/169.html">Jesper Östman</a></strong> on 2011-01-31T14:44:00</p>
<div class="content"><div class="postrev" data-snap="0">DanielLC:<br/><br/><span style="font-weight: bold">About the doomsday argument: </span><br/>It is an interesting challenge for the importance of existential risk reduction, since unlike other challenges (eg that space colonization might be hard or impossible) it makes future scenarios more unlikely the more people they contain. Because of this, where most other challenges would, if succesful, only reduce reduce the likelihood of a huge future population (and thus the expected utility from astronomical risk reduction) by a few orders of magnitude the doomsday argument could potentially bring the astronomical expected utility down to an "earthly" size. <br/><br/>However, we must also take into account the probability of the doomsday argument being incorrect. Since it is controversial I wouldn't assign a probability higher than 0.5 to it. But even if one is fairly convinced that the argument is correct a certainty far above 0.99 seems unmerited. Thus, the doomsday argument would have the same effect as other challenges and at worst bring down the expected utility by a few orders of magnitude, keeping it astronomically huge.<br/><br/>[1] This reasoning is analogous to the theory uncertainty arguments employed by <a class="postlink" href="http://arxiv.org/ftp/arxiv/papers/0810/0810.5515.pdf">Ord et al</a> to show that potential existential risks from physics experiments cannot be completely ignored even if we have good arguments purporting to show that they are impossible.</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2580">
<dt>
<a href="../user/169.html"></a><br/>
<a href="../user/169.html">Jesper Östman</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 159</dd>
<dd><strong>Joined:</strong> Mon Oct 26, 2009 5:23 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2581">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2581">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/169.html">Jesper Östman</a></strong> on 2011-01-31T15:50:00</p>
<div class="content"><div class="postrev" data-snap="0">Arepo:<br/>Which of Bostrom's premises do you doubt, and with roughly what certainty?<br/><br/><span style="font-weight: bold">Existential risk reduction vs meme promotion</span><br/>1) If I'm understanding your argument correctly it is that meme-promotion can be as effective, or even more effective than, existential risk reduction. I completely agree, personally I believe that the two most important utilitarian issues are existential risk reduction and promotion of memes which will increase the probability of a more happy future. Examples of such memes would be (1) "without conscious happiness there is no value" , (2) "The experiences of all sentient beings (eg. farm animals, wild animals, AIs) are of comparable value" (3) "The more happiness/happy beings, the better".<br/><br/>Why these memes? (1)  would be a way to reduce existential <a class="postlink" href="http://www.nickbostrom.com/fut/evolution.html">risks from future human evolution</a> (we could also describe it as a way of ensuring that the hedons keep reproducing in the long run) where non-conscious or fairly unhappy beings outcompe the happy beings. (2) is there to avoid scenarios where the astronomical amounts of future humans create similar amounts of suffering farm/wild animals &lt;see my post below for a more comprehensive treatment of this issue&gt; (3) is there to avoid scenarios where humans don't colonize space optimally.<br/><br/>Note that the concerns of meme promoting and existential risk reduction depend on each other for their value. The spread of happiness wouldn't matter much if we all die in 50 years. On the other hand, without space expansion, pleasurable consciousness or with proportional animal suffering it wouldn't matter much if the descendants of humanity survives for billions of years.<br/><br/>Since the concerns are very interrelated and overlapping what is risk-reduction and what is happiness promotion is (at least for a utilitarian, who could see scenarios where humanity survives but don't maximize happiness as existential risks) more or less a question of giving useful stipulative definitions.<br/><br/>What should we focus on in practice? This is a hard question obviously, which depends on a lot of empirical details. Generally, it would seem rational to use one's resources where they would make the biggest impact on the probability of the happy space colonization. Some existential risk reduction may be low hanging fruit (eg more ER-research, implementing regulations to prevent synthesizing of genetically engineered pathogens). The efficiency of meme-promotion and the respective importance of promoting eg (1)-(3) is harder to evaluate without a rigorous science of meme-promotion. Perhaps promoting the spread of happiness compared to other "dangerous" values such life/replication is most important since it seems likely that we won't have a lot of suffering animals around in the far future, or that we would forego a continuing space expansion.</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2581">
<dt>
<a href="../user/169.html"></a><br/>
<a href="../user/169.html">Jesper Östman</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 159</dd>
<dd><strong>Joined:</strong> Mon Oct 26, 2009 5:23 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2582">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2582">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/169.html">Jesper Östman</a></strong> on 2011-01-31T16:52:00</p>
<div class="content"><div class="postrev" data-snap="0">Arepo:<br/>According to the experts I've heard, and my own judgment, the anthropogenic risks seem much bigger than all the natural risks taken together.<br/><br/>I used the asteroid/comet risk as an example because it's a case where we relatively easily can get empirically supported probabilities and where we know of concrete countermeasures. The aim was to get a baseline case showing that even relatively inefficient ways of reducing risk are astronomically cost-effective. So if the alternative is doing nothing at all, it would be very worthwhile to use one's resources to promote asteroid defence. If our alternatives aren't constrained in that way it would be a superior alternative to focus on mitigating the anthropogenic risks.<br/><br/><span style="font-weight: bold">Efficiency of ordinary causes for risk reduction</span><br/>I agree that reducing global conflicts and also climate change is very important. The main reason for favoring non-standard causes isn't that the other projects are unimportant but that so much resources are already spent on them. A couple million dollars could for example perhaps double the existential risk research field whereas it would be a drop in the ocean when it comes to global poverty reduction, work against climate change or work for peace/nuclear disarmament.<br/><br/>For example, many are skeptical about whether foreign aid has any positive effect at all. However, I do faintly recall a world bank report claiming that each billion spent would roughly increase the GNP of a normal developing country by 1%. Assuming the skeptics are incorrect and that optimal GWWC targeted charities are 500 times more effective than government aid then we could get a 1% increase from a couple million dollars. That would likely at best reduce global risk marginally.<br/><br/><span style="font-weight: bold">Promoting utilitarianism, or just important related memes?</span><br/>When it comes to promoting utilitarianism, I think the best strategy would be to promote the components of utilitarianism which are most important for utilitarians. To see this, let us compare three scenarios.<br/>In (A) the world is governed according to common sense ethics, in (B) according to a mix of common sense ethics and the important parts of utilitarianism and in (C) according to pure utilitarianism. Reasonably, assuming the corresponding worlds are shaped according to the ethical positions, the (C) and (B) worlds will contain similar astronomical amounts of happiness, whereas (A) would be relatively worthless. Now, given that utilitarianism is very counter-intuitive for most people (largely because of a bunch of more or less good intuitive objections which a mixed position could avoid) it would seem far easier and more cost effective to aim at moving people's values towards the values (B) rather than pure utilitarianism.<br/><br/>What would such a mixed position look like? Let us assume the mixed view contains (or has as consequences) the position the content of the memes (1)-(3). So happy experiences are necessary for value, and we get more value the more such we have (ideally without any discounting for time/space/amount). Animal experiences are valuable, although perhaps somewhat less so than human experiences. Now the view contains most important utilitarian components. In addition the position could hold common sense views such that sadistic pleasure, and pleasure based on illusion is worthless. This would directly avoid the purported gang-rape and experience machine counter-examples against utilitarianism. Furthermore lots of things could bear value, as long as this is small compared to happy/painful experiences: eg nature, beautiful things, complexity and whatnot. The important thing is that the pleasures we de-value wouldn't likely form an essential part of the happiness in a happy cosmic expansion scenario and that the additional things besides happiness that we value either don't conflict with this goal, or likely won't reach astronomical values. Perhaps we could even include fairness of distribution (an unfair distribution doesn't seem essential to space expansion) and some punishment/desert (the losses here would be low compared to the total happiness). It could even be possible to include some watered-down deontology (and the deontology of common people is watered down), eg that it is wrong to kill, as long as the gains aren't huge (saving 100 happy people eg).</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2582">
<dt>
<a href="../user/169.html"></a><br/>
<a href="../user/169.html">Jesper Östman</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 159</dd>
<dd><strong>Joined:</strong> Mon Oct 26, 2009 5:23 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2583">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2583">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/169.html">Jesper Östman</a></strong> on 2011-01-31T17:30:00</p>
<div class="content"><div class="postrev" data-snap="0"><span style="font-weight: bold">(II) The happiness assumption: future animals<br/></span>Returning to RyanCareys concern that future humans might detract from the amount of happiness in the universe. In a reply above I argued that the humans themselves will likely be very happy. Here I will investigate whether they are likely cause huge amounts of suffering to non-human creatures. I see two main scenarios where future humans would cause in the first (S1) for billions of years the humans keep farming and eating animals, which are raised under or similar or worse conditions than contemporary factory farming. In the second (S2) they keep huge amounts of (by assumption) suffering wild-life. It is not just that wild animals and farm animals are kept alive on earth, but they are brought to each or most of all the billion of new colonies. <br/><br/>Personally, I think it is unlikely that farm animals will be kept around for that long for a few reasons. Advanced in-vitro-meat will likely be developed at least within a thousand years. Such meat would be superior to meat from farmed animals in several ways: (1) it would be a lot cheaper and more resource effective (2) it would taste better (3) it would be healthier (4) it would be seen as more ethical. The better technology we get the more important (1) would be, since the matter and energy used for raising farm animals could be used for much more valuable things (eg nano-tech, super-computers, robotics). The only reasons for paying such a high price for keeping animals around would be either very strong preferences for traditional living (which doesn't seem likely in a space-faring civilisation) or for making animals suffer. <br/><br/>Furthermore, if humans upload, or the overwhelming majority of sentient beings become non-biological in some other way that would mean a prompt end to meat consumption.<br/><br/>What about wild animals? <a class="postlink" href="http://www.utilitarian-essays.com/lab-universes.html">The main risks</a> here are perhaps that future humans would spread huge amounts of (largely suffering) wild-life to the new star-systems they colonize or, assuming futuristic physics, create new universes filled with life. If one finds such scenarios likely it would perhaps be more valuable to  <a class="postlink" href="../thread/245.html">spread memes</a> about the importance of wild animals suffering - or about theodicy-like responsibilities of not creating huge amounts of suffering lives. Or risk reduction should be complemented by such meme promotion.<br/><br/>Personally, I find it unlikely that future humans would create astronomical amounts of wildlife for similar reasons I don't believe they'd eat meat. (1) it would extremely expensive to use whole planets as wild-life parks (2) simulations could give a superior, and astronomically cheaper, experience of wildlife. <br/><br/>The second concern here, (2), introduces a new and perhaps more likely problem. It is possible that future beings would keep around massive simulations which could potentially be filled with suffering minds for entertainment, experiment or other purposes. The same remedies as for the wild-animal case are likely the way to go here.</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile2583">
<dt>
<a href="../user/169.html"></a><br/>
<a href="../user/169.html">Jesper Östman</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 159</dd>
<dd><strong>Joined:</strong> Mon Oct 26, 2009 5:23 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p2754">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p2754">Re: Existential risk reduction cost effectiveness</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/567.html">Hedonic Treader</a></strong> on 2011-04-20T20:32:00</p>
<div class="content"><div class="postrev" data-snap="0">Some points that may affect the probability of the happiness assumption: <br/><br/>I'd suspect a) conflicts over limited resources, and b) power concentration as abstract problems with the potential to cause amounts of suffering that may scale with population and complexity, including complexity of available technology.<br/><br/>a) The conflict problem is exemplified in predator/prey relationships in ecosystems as well as in wars between tribes, nations, and maybe future interstellar entities. The future risks here lie in astronomically large-scaled war or game-theoretic arms races like mutually assured mass supertorture between conflicting entities. The probability of such scenarios may be higher than we think. The beginning of colonization may be a crucial tipping point in the future, deciding whether or not conflicting large-scale entities can come into existence and what degree of evolutionary freedom vs. enforced coordination applies from the start.<br/><br/>Potential anhedonic selection pressures between replicating entities such as uploads, clones etc. might also occur in any system that doesn't suppress free evolution and allows replicating entities to reach carrying capacity of the resource base, given the assumption that there's even the slightest adaptive advantage in experiencing less-than-zero affect when losing out on replication resources. Once we understand how minds in general work, it may be worthwhile to find out if there could be mind designs that always produce net-positive observer moments despite this problem, and whether evolutionary pathways leading away from this design principle can be somehow blocked on a fundamental level (improbable, I would assume).<br/><br/>b) The power concentration problem is exemplified in dictatorships and oligarchies, as well as factory farming. Equivalent future risks here would come from a totalitarian anhedonic expansion process that holds a tight technology-driven grip on the sentients that are forced to be components of the system. The probability of this seems relatively low since mind-control technologies could actually make torture obsolote in many ways. An expansion system of disposable superhappy slaves without personal rights may sound dystopic, but the utility landscape may still be net-positive - unless the decision-making entities of the system are sadistic or indifferent.<br/><br/>Sadism seems unlikely unless an initial lock-in is originated by a dominant alpha-male psychology that represents the causation of suffering as a symbol of hierarchical status, which is valued. Absolute indifference to the well-being of other sentients is also not terribly likely since the decision-making entities will initially originate from human systems, and almost all humans have at least some level of benevolence bias, all other things considered equal. However, this point may be undermined if initial hedonic enhancement of decision-making entities strips them of vital aspects of compassion - e.g. if they are unable to comprehend that observer moments can be below zero hedonistically, they may not intuitively understand the necessity of preventing certain mental states at all while designing their de-facto totalitarian system. This may also lead to a scenario where they create pocket universes or sentient simulations containing suffering, without being successfully convinced by memes that highlight the ethical importance of not creating them.</div><div class="diff hidden"></div></div>
<div class="signature">"The abolishment of pain in surgery is a chimera. It is absurd to go on seeking it... Knife and pain are two words in surgery that must forever be associated in the consciousness of the patient."<br/><br/>- Dr. Alfred Velpeau (1839), French surgeon</div>
</div>
<dl class="postprofile" id="profile2754">
<dt>
<a href="../user/567.html"><img alt="User avatar" height="100" src="../file/567_1355975427.jpg" width="100"/></a><br/>
<a href="../user/567.html">Hedonic Treader</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 342</dd>
<dd><strong>Joined:</strong> Sun Apr 17, 2011 11:06 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<input type="hidden" value="20111005044054/viewtopic.php?f=23&amp;t=348"/>
                        <hr>
                        <div class="topic-actions">
                            <div class="pagination">
                                13 posts
                            </div>
                        </div>
                        <p><a href="./viewforum.php?f=10" class="left-box left" accesskey="r">Return to General discussion</a></p>
                    </div>
                    <div id="page-footer">
                    </div>
                </div>
                <div>
                    <a id="bottom" name="bottom" accesskey="z"></a>
                </div>
            </div>
            <div class="positioncorrection-bottom"></div>
        </div>
        <div class="bottom-left"></div>
        <div class="bottom-middle"></div>
        <div class="bottom-right"></div>
    </body>
</html>
