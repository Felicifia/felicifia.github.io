<!doctype html>
<html>
    <head>
        <meta charset="utf-8">
        <title>Felicifia: global utilitarian discussion &bull; View topic - Implications of Evidential Decision Theory</title>
        <script type="text/javascript" src="../styles/nexus/template/styleswitcher.js"></script>
        <script type="text/javascript" src="../styles/nexus/template/forum_fn.js"></script>
        <script type="text/javascript" src="../styles/custom.js"></script>
        <link href="../styles/nexus/theme/print.css" rel="stylesheet" type="text/css" media="print" title="printonly"/>
        <link href="../styles/prosilver.css" rel="stylesheet" type="text/css" media="screen, projection"/>
        <link href="../styles/nexus/theme/normal.css" rel="stylesheet" type="text/css" title="A"/>
        <link href="../styles/nexus/theme/medium.css" rel="alternate stylesheet" type="text/css" title="A+"/>
        <link href="../styles/nexus/theme/large.css" rel="alternate stylesheet" type="text/css" title="A++"/>
        <link href="../styles/custom.css" rel="stylesheet" type="text/css"/>
        <script type="text/javascript"><!--
            var spoiler_show = "[Reveal]";
            var spoiler_hide = "[Obscure]";
            //-->
        </script>
        <script type="text/javascript" src="../styles/nexus/template/prime_bbcode_spoiler.js"></script>
        <link href="../styles/nexus/theme/prime_bbcode_spoiler.css" rel="stylesheet" type="text/css"/>
    </head>
    <body id="phpbb" class="section-viewtopic ltr">
        <div id="mainframe">
        <div class="top-left"></div>
        <div class="top-middle"></div>
        <div class="top-right"></div>
        <div class="inner-wrap">
            <div class="positioncorrection-top">
                <div id="wrap">
                    <a id="top" name="top" accesskey="t"></a>
                    <div id="page-header">
                        <div class="headerbar">
                            <div class="inner">
                                <span class="corners-top"><span></span></span>
                                <div id="site-description">
                                    <a href="../forum/index.html" title="Board index" id="logo"><img src="../styles/nexus/imageset/simple%20logo.png" alt="" title="" width="766" height="126"></a>
                                    <p style="display: none;"><a href="#start_here">Skip to content</a></p>
                                </div>
                                <span class="corners-bottom"><span></span></span>
                            </div>
                        </div>
                        <div class="navbar">
                            <div class="inner">
                                <span class="corners-top"><span></span></span>
                                <ul class="linklist navlinks">
                                    <li class="icon-home"><a href="../forum/index.html" accesskey="h">Board index</a>  <strong>‹</strong> <a href="../forum/29.html">Utilitarian future</a></li>
                                    <li class="rightside"><a href="#" onclick="fontsizeup(); return false;" onkeypress="fontsizeup(); return false;" class="fontsize" title="Change font size">Change font size</a></li>
                                </ul>
                                <span class="corners-bottom"><span></span></span>
                            </div>
                        </div>
                    </div>
                    <!--
                        <div class="google">

                        </div>
                        -->
                    <a name="start_here"></a>
                    <div id="page-body">
                        <h2><a href="#">Implications of Evidential Decision Theory</a></h2>
                        <!-- NOTE: remove the style="display: none" when you want to have the forum description on the topic body --><span style="display: none">Whether it's pushpin, poetry or neither, you can discuss it here.<br></span>
                        <div class="topic-actions">
                            <div class="buttons">
                            </div>
                            <div class="pagination">
                                25 posts
                            </div>
                        </div>
                        <div class="clear"></div>
                        <div class="post bg2" id="p7652">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7652">Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/64.html">Brian Tomasik</a></strong> on 2013-02-13T04:55:00</p>
<div class="content"><div class="postrev" data-snap="1">Paul Almond is one of the smartest people I know, and I need to get in the habit of reading more of <a class="postlink" href="http://www.paul-almond.com/">his essays</a>. One that I came across tonight was "<a class="postlink" href="http://www.paul-almond.com/Correlation2.pdf">On Causation and Correlation - Part 2: Implications of Evidential Decision Theory</a>." I only skimmed it for now and would like to read it properly some time later, but I thought I would write quick notes about it in case that "some time later" never seems to happen.<br/><br/>I was interested to learn that Paul subscribes to <a class="postlink" href="http://en.wikipedia.org/wiki/Evidential_decision_theory">evidential decision theory</a> (EDT) in a similar way as I do. In the past, I had been a regular <a class="postlink" href="http://en.wikipedia.org/wiki/Causal_decision_theory">causal decision theorist</a> (CDT), as most people are, but upon learning about EDT, I became more uncertain. [BTW, I wrote those linked Wikipedia articles on EDT and CDT in 2009. If you can believe it, there were no Wikipedia articles for these terms before then.]<br/><br/>In most of the standard thought experiments, I side with the EDT'ers. Even for the <a class="postlink" href="http://wiki.lesswrong.com/wiki/Smoking_lesion">smoker's lesion problem</a>, where the "correct" answer is usually that you should smoke because you can't change your genes, I'm uncertain what I would do.<br/><br/>I don't know if I understand <a class="postlink" href="http://wiki.lesswrong.com/wiki/Timeless_decision_theory">timeless decision theory</a> (TDT) properly, but if I do, then I think TDT is basically EDT applied only to your cognitive algorithms, not to everything. That is, if you choose X, it gives you evidence that the general cognitive algorithm you're running tends to output X. But it doesn't extend to non-cognitive parts of the universe. I would ask, "Why not? Your genes are another type of algorithm. Why not use EDT for them too?"<br/><br/>Anyway, I should read more of what Paul has to say on this later.<br/><br/>As for the article itself, it explains some really important ideas, a few of which I and friends have discussed informally but haven't written down systematically. There are many interesting points, but one that I wanted to highlight is the following. It suggests why, even for our own sake, we should ensure that sims are treated nicely and that religious fundamentalists don't take control of massive computational resources:<br/><blockquote class="uncited"><div>One way in which evidential decision theory would be relevant is in the way it allows<br/>you to control the probability that you are in a simulation in the first place. If your <br/>civilization decides to develop the capability to run simulated realities, then you are <br/>meta-causing civilizations in general to do likewise (including civilizations on which our <br/>own might be modeled), and making it less likely that almost all civilizations end before <br/>they are capable of producing simulated realities, in turn making it more likely that you<br/>are in a simulated reality. If, however, your civilization decides  not to acquire this <br/>capability then you are meta-causing civilizations in general to do likewise, making it less <br/>likely that you are in a simulated reality. Once your civilization has the capability to <br/>produce simulated realities, if your civilization decides to do it, this would make it more <br/>likely that other civilizations also do it, again making it more likely that you are in a <br/>simulated reality. On the other hand, if your civilization decides not to produce <br/>simulated realities, this makes it less likely that other civilizations would choose to do <br/>so, and therefore less likely that you are in a simulated reality yourself. [...]<br/><br/>Evidential decision theory is not restricted to the issue of whether we are in a simulated <br/>reality. If we are in a simulated reality, it might be relevant in allowing us to control the <br/>probabilities that we are in various kinds of simulation. If we construct many simulated <br/>realities in which various things happen, then if another civilization is simulating us, we <br/>might be meta-causing it to make those things happen to us. This creates an argument <br/>for being kind to the inhabitants of any simulated realities that you do make.<br/></div></blockquote></div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile7652">
<dt>
<a href="../user/64.html"><img alt="User avatar" height="100" src="../file/64_1317625384.jpg" width="100"/></a><br/>
<a href="../user/64.html">Brian Tomasik</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1130</dd>
<dd><strong>Joined:</strong> Tue Oct 28, 2008 3:10 am</dd>
<dd><strong>Location:</strong> USA</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7655">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7655">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-02-13T07:00:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote class="uncited"><div>That is, if you choose X, it gives you evidence that the general cognitive algorithm you're running tends to output X.</div></blockquote><br/><br/>That's not how I understand it.<br/><br/>You ignore all evidence and work only from your priors and the knowledge that in that situation you'd make whatever decision you make. You don't involve the fact that you actually are in that situation. For example, in Parfit's hitchhiker, you'd never be able to decide whether or not to pay the guy unless he picked you up, but you don't take this into account. You simply decide that, a priori, a universe where you would pay him is better than one where you would not.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7655">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7703">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7703">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/64.html">Brian Tomasik</a></strong> on 2013-02-17T00:44:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>DanielLC wrote:</cite>You simply decide that, a priori, a universe where you would pay him is better than one where you would not.</div></blockquote><br/>A universe where you pay him <span style="font-style: italic">in this situation</span> is better. Hence, you're saying you'd like to find it the case that you decide to pay him. If you do resolve in your own mind that you want to pay him, that's evidence that you're the kind of person who would pay in situations like these, so he's more likely to believe you and accept the deal. Do we disagree?</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile7703">
<dt>
<a href="../user/64.html"><img alt="User avatar" height="100" src="../file/64_1317625384.jpg" width="100"/></a><br/>
<a href="../user/64.html">Brian Tomasik</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1130</dd>
<dd><strong>Joined:</strong> Tue Oct 28, 2008 3:10 am</dd>
<dd><strong>Location:</strong> USA</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7712">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7712">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-02-18T07:03:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote class="uncited"><div>If you do resolve in your own mind that you want to pay him, that's evidence that you're the kind of person who would pay in situations like these, so he's more likely to believe you and accept the deal.</div></blockquote><br/><br/>That's not the decision the paradox is about. The paradox is about the decision of whether or not to pay him once he has already rescued you. Once this happens, no choice you make can provide evidence that he rescued you, since it's certain he has no matter what you decide to do.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7712">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7722">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7722">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/64.html">Brian Tomasik</a></strong> on 2013-02-19T08:48:00</p>
<div class="content"><div class="postrev" data-snap="1">What I meant was you want to discover that you have the kind of cognitive algorithm that choose to adopt the policy of paying him iff he rescues you. In other words, you want to discover that you're someone who keeps promises. If you are, then he'll save you. So you'd like to try your best to make yourself commit to promise-keeping in whatever forms that takes.</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile7722">
<dt>
<a href="../user/64.html"><img alt="User avatar" height="100" src="../file/64_1317625384.jpg" width="100"/></a><br/>
<a href="../user/64.html">Brian Tomasik</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1130</dd>
<dd><strong>Joined:</strong> Tue Oct 28, 2008 3:10 am</dd>
<dd><strong>Location:</strong> USA</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7732">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7732">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-02-20T07:31:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote class="uncited"><div>So you'd like to try your best to make yourself commit to promise-keeping in whatever forms that takes.</div></blockquote><br/><br/>You'd do that in any decision theory. It's not an interesting problem. The interesting problem is when your promise is held due.<br/><br/>Also, there are versions where you don't get a chance to make a promise. Perhaps you were already passed out when he picked you up, but he knew you well enough to know you'd pay.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7732">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7733">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7733">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/64.html">Brian Tomasik</a></strong> on 2013-02-20T09:06:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>DanielLC wrote:</cite>You'd do that in any decision theory. It's not an interesting problem. The interesting problem is when your promise is held due.</div></blockquote><br/>Interesting. I guess what I meant was that even if you have no physical pre-commitment mechanisms, you want to find that your choice would be to pay him if you were to imagine the situation happening. I think that's what you meant in your first reply.</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile7733">
<dt>
<a href="../user/64.html"><img alt="User avatar" height="100" src="../file/64_1317625384.jpg" width="100"/></a><br/>
<a href="../user/64.html">Brian Tomasik</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1130</dd>
<dd><strong>Joined:</strong> Tue Oct 28, 2008 3:10 am</dd>
<dd><strong>Location:</strong> USA</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7737">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7737">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2013-02-20T12:37:00</p>
<div class="content"><div class="postrev" data-snap="1">To both of you: what is your objection to the non-DT response to the hitchhiker scenario that how well you can persuade the driver that you’ll give him money is not necessarily related to your actual likelihood of doing so?</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile7737">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7741">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7741">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-02-20T22:15:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote class="uncited"><div>I guess what I meant was that even if you have no physical pre-commitment mechanisms, you want to find that your choice would be to pay him if you were to imagine the situation happening.</div></blockquote><br/><br/>You would, and you'd be more likely to pay him because of it. If this is something that's going to happen on a regular basis, you'd pay him, knowing that if you don't you won't pay the next guy and he'll call your bluff. If it's not likely to come up again, then you're probably not going to pay him, assuming you're using EDT.<br/><br/><blockquote class="uncited"><div>what is your objection to the non-DT response to the hitchhiker scenario that how well you can persuade the driver that you’ll give him money is not necessarily related to your actual likelihood of doing so?</div></blockquote><br/><br/>If there's no correlation, it's a boring problem. We only discuss the more interesting variant in which there is a correlation.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7741">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7743">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7743">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/64.html">Brian Tomasik</a></strong> on 2013-02-20T22:40:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>Arepo wrote:</cite>To both of you: what is your objection to the non-DT response to the hitchhiker scenario that how well you can persuade the driver that you’ll give him money is not necessarily related to your actual likelihood of doing so?</div></blockquote><br/>OLD REPLY: I think that's actually a DT-dependent response. To a causal decision theorist, once you were already rescued, then you would have no incentive to pay (ignoring reputation effects, repeated plays of the game, etc.). An evidential/timeless decision theorist would, I think, say that you want to find that your algorithm decides to pay even after it has been rescued, because this is the algorithm that lets you win.<br/><br/>NEW REPLY: Sorry, I missed the "not" in your "not necessarily" clause. Ok, in that case I agree with DanielLC that it's an uninteresting problem. (It's also not true that your persuasion abilities are not necessarily related to your choice when the driver has your source code and can simulate what you would do in the situation.)</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile7743">
<dt>
<a href="../user/64.html"><img alt="User avatar" height="100" src="../file/64_1317625384.jpg" width="100"/></a><br/>
<a href="../user/64.html">Brian Tomasik</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1130</dd>
<dd><strong>Joined:</strong> Tue Oct 28, 2008 3:10 am</dd>
<dd><strong>Location:</strong> USA</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7744">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7744">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/64.html">Brian Tomasik</a></strong> on 2013-02-20T22:53:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>DanielLC wrote:</cite>If it's not likely to come up again, then you're probably not going to pay him, assuming you're using EDT.<br/></div></blockquote><br/>OLD REPLY:<br/><br/>I don't see this. I think EDT would say that when I inspect my own thoughts about whether to pay him later, I'd like to find that I would pay him later, because discovering this makes it more likely I would in fact pay him later. <br/><br/>Recall that the literature conventionally says that EDT one-boxes on Newcomb, so I think the standard interpretation of EDT would win on Parfit's hitchhiker too.<br/><br/>NEW REPLY:<br/><br/>Hmm, now I see where DanielLC was coming from. Newcomb is potentially different from hitchhiker because in Newcomb, there's no time delay. When you choose to one-box, you get $1 million. In hitchhiker, it's conceivable that your choice could change between when you promise to pay and once you're actually rescued. As DanielLC said, once you're rescued, that's all the evidence you need that you were rescued, so you don't also need to find that you would pay out.<br/><br/>So maybe we need a cognitive-algorithm-focused spin on EDT sometimes, and that's what TDT is, AFAICT.<br/><br/>P.S.: I'm updating my replies a lot in part because I'm still confused about decision theory in general. This is a good learning exercise for me.</div><div class="diff hidden"></div></div>
<div class="signature"></div>
</div>
<dl class="postprofile" id="profile7744">
<dt>
<a href="../user/64.html"><img alt="User avatar" height="100" src="../file/64_1317625384.jpg" width="100"/></a><br/>
<a href="../user/64.html">Brian Tomasik</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1130</dd>
<dd><strong>Joined:</strong> Tue Oct 28, 2008 3:10 am</dd>
<dd><strong>Location:</strong> USA</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7746">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7746">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2013-02-21T14:41:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>Daniel &amp; Alan wrote:</cite>If there's no correlation, it's a boring problem. We only discuss the more interesting variant in which there is a correlation.<br/><br/>...<br/><br/>NEW REPLY: Sorry, I missed the "not" in your "not necessarily" clause. Ok, in that case I agree with DanielLC that it's an uninteresting problem. (It's also not true that your persuasion abilities are not necessarily related to your choice when the driver has your source code and can simulate what you would do in the situation.)<br/></div></blockquote><br/><br/>I wholeheartedly agree it’s uninteresting in this case. Our dispute is whether it becomes interesting elsewhere.<br/><br/><blockquote class="uncited"><div>OLD REPLY: I think that's actually a DT-dependent response. To a causal decision theorist, once you were already rescued, then you would have no incentive to pay (ignoring reputation effects, repeated plays of the game, etc.). An evidential/timeless decision theorist would, I think, say that you want to find that your algorithm decides to pay even after it has been rescued, because this is the algorithm that lets you win.</div></blockquote><br/><br/>If you specify the situation such that intent and appearance don’t correlate, then me paying is inconsistent with the premise that I’m a rational self-utility maximiser (RSUM). Obviously you can call the not-paying consequence of me being an RSUM ‘[some particular DT]’ if you like, but I don’t see what you stand to gain.<br/><br/>If you specify the situation such that intent and appearance correspond, then me not intending to pay is inconsistent with the premise that I’m an RSUM.<br/><br/>If you specify a middle ground, such that I have positive expectation of the correlation between intent and appearance, then I run a utility calculation. If [the number of years of l expect to live on escaping the desert] * [my utility per year] * [the difference in probability I think sincerely vowing would make] is greater than the utility I expect the money he’s asking for would buy me conditional on my survival, then I vow sincerely, else I don’t.<br/><br/>But of the above three situations, only the first (and perhaps a very weak version of the second, such that I have an easy choice to not vow) seems at all plausible in this universe. The others remind me of Hare’s ‘how to argue with an anti-utilitarian’. They are fantastic examples disguised in real-world clothing, inadmissible as appeals to intuition, because they don’t contain anywhere near enough detail. <br/><br/>Firstly, how do we know – or have any idea of - the driver’s ability to perceive my intent? As in Newcomb’s paradox, the basic problem assumes we have just been given some sourceless knowledge which we’re expected to take at face value.<br/><br/>Secondly, why – other than a long tradition of abusing the term – are we so sure ‘intent’ refers to a genuine (apparently emergent) phenomenon? It seems to me near perfectly reducible to the feelings evoked when we contemplate certain situations, and where it’s imperfectly so, only because of its vagary; people occasionally slip in nuances when they use it.<br/><br/>If we so clarify ‘intent’, the emptiness of the ‘problem’ becomes even more apparent. Perhaps I can change the chemistry of my brain by willpower alone such that future-me becomes more likely to cooperate with present-me, and if so not doing so is inconsistent with the premise that I’m an RSUM. <br/><br/>If I can’t then clearly I don’t and admittedly now ‘I’ die because ‘I’ was ‘too rational’. But this isn’t biting any bullet – this is just looking through the range of logically conceivable outcomes and noting that I can’t guarantee winning the game. If I don’t have the requisite self-modifying ability, I lose. So what? I see no paradox in this, nor the need to consider anything other than expected value calculations to reach this point, and nothing that importing any further concepts from ‘decision theory’ would do to improve my chances of winning.<br/><br/>(Incidentally the deflationary view of personal identity - which I’d more aggressively refer to as its deflationary nature, since any alternative seems ludicrous -  makes this entire discussion empty on its original premises. There was no essential ‘me’ for a perfectly logical creature to want to preserve – the tension between present-me and future-me just showcases that – so the scenario is incoherent. <br/><br/>I might be able to ironman the setup by restating it such that I’m a utilitarian and need to survive for the greater good, but know that money given to the driver will be wasted, in which case I think the scenario just about makes logical sense. But as above, I don’t see a paradox or problem, per se. I play the game with the best strategies available to me given the rules, and if I lose, so be it.)</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile7746">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7754">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7754">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-02-22T07:04:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote class="uncited"><div>Our dispute is whether it becomes interesting elsewhere.</div></blockquote><br/><br/>When would it be interesting?<br/><br/><blockquote class="uncited"><div>Obviously you can call the not-paying consequence of me being an RSUM ‘[some particular DT]’ if you like, but I don’t see what you stand to gain.</div></blockquote><br/><br/>As is the case with many uninteresting problems. If you see a winning lottery ticket on the ground, should you pick it up? If you're standing next to a cliff, should you jump off?<br/><br/><blockquote class="uncited"><div>If you specify a middle ground, such that I have positive expectation of the correlation between intent and appearance, then I run a utility calculation. If [the number of years of l expect to live on escaping the desert] * [my utility per year] * [the difference in probability I think sincerely vowing would make] is greater than the utility I expect the money he’s asking for would buy me conditional on my survival, then I vow sincerely, else I don’t.</div></blockquote><br/><br/>Your choice of whether or not the vow is sincere, i.e. whether or not you fulfill it, is made after you've made the vow and you were picked up. How do you justify a nonzero difference in probability of survival after you know you've been picked up?<br/><br/><blockquote class="uncited"><div>Firstly, how do we know – or have any idea of - the driver’s ability to perceive my intent?</div></blockquote><br/><br/>You could tell by his track record. You could also just be a bad liar. In this case, you could think of it as past!you trying to tell if future!you will keep the promise. Past!you knows you very, very well, and is likely to make a good prediction.<br/><br/><blockquote class="uncited"><div>Secondly, why – other than a long tradition of abusing the term – are we so sure ‘intent’ refers to a genuine (apparently emergent) phenomenon?</div></blockquote><br/><br/>We know that either you will fulfill your promise or you will break it. We presumably know that the other guy can predict whether or not it will be broken at a rate significantly above chance.<br/><br/><blockquote class="uncited"><div>this is just looking through the range of logically conceivable outcomes and noting that I can’t guarantee winning the game.</div></blockquote><br/><br/>The strategy of paying the guy yields a significantly higher success rate than the strategy of not paying him. Is this not logically conceivable? Do you feel that because you can't be certain that following your strategy will lead to a worse outcome, it doesn't matter that it usually does?<br/><br/><blockquote class="uncited"><div>There was no essential ‘me’ for a perfectly logical creature to want to preserve</div></blockquote><br/><br/>There was still someone. If you value people in general, you'd want him to not die. Also, he's quite a lot like you, and he will likely bring what you value.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7754">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7760">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7760">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2013-02-22T14:13:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>DanielLC wrote:</cite>When would it be interesting?</div></blockquote><br/><br/>It would be interesting to me (in this context) iff some aspect of decision theory (that did not conceptually predate the naming of the decision theory under which it fell) could improve the analysis.<br/><br/><blockquote class="uncited"><div>Your choice of whether or not the vow is sincere, i.e. whether or not you fulfill it, is made after you've made the vow and you were picked up. How do you justify a nonzero difference in probability of survival after you know you've been picked up?</div></blockquote><br/><br/>Not entirely. If I make the vow imagining that I’ll keep it, something different has happened from whether I make the vow imagining that I’ll break it. But if not, and if we’re still claiming that the driver’s behaviour is linked to whether future-me keeps it despite no difference in present-me, then we’re in Omega territory – a world so alien that anything goes and nothing from it matters in this one.<br/><br/><blockquote class="uncited"><div>You could tell by his track record. You could also just be a bad liar. </div></blockquote><br/><br/>There are many ways in which you *could*, but if the thought experiment is supposed to change our intuition, it's worthless unless we demand high precision of it. Not an ad hoc set of possible justifications for the wild claims, but an actual scenario up front, described with all the salient details, where demonstrating an unrealistic feature would render the whole thing irrelevant to our intuitions.<br/><br/><blockquote class="uncited"><div> <blockquote class="uncited"><div>Secondly, why – other than a long tradition of abusing the term – are we so sure ‘intent’ refers to a genuine (apparently emergent) phenomenon?</div></blockquote><br/><br/>We know that either you will fulfill your promise or you will break it. We presumably know that the other guy can predict whether or not it will be broken at a rate significantly above chance.</div></blockquote><br/><br/>What does this have to do with my criticism of ‘intent’? Whatever we refer to by the word, I think we can agree that ‘I intend x’ isn’t logically equivalent to ‘X will happen’.<br/><br/><blockquote class="uncited"><div>The strategy of paying the guy yields a significantly higher success rate than the strategy of not paying him. Is this not logically conceivable? Do you feel that because you can't be certain that following your strategy will lead to a worse outcome, it doesn't matter that it usually does?</div></blockquote><br/><br/>What on earth makes you say it ‘usually leads to a worse outcome’? My ‘strategy’ is basically defined as being the best approach: ‘Maximise expected value’. It deals with all the possible outcomes with higher expected value than any different strategy. In some scenarios it will mean paying, in most it won’t, and I don’t need a ‘decision theory’ to cover each (or really any) of those possibilities. <br/><br/>If I ever lose, it’s only because I’ve taken a maximal-expectation gamble and it hasn’t paid off. So what? I’d do the same again any time I had the chance.<br/><br/><blockquote class="uncited"><div>There was still someone. If you value people in general, you'd want him to not die. Also, he's quite a lot like you, and he will likely bring what you value.</div></blockquote><br/><br/>If I’m a rational pure self-interest-maximiser, the only thing I value is me, so since he won’t be me a) he won’t value the same thing and b) it wouldn’t make a difference if he did, because now-me would still be gone. As I said, I think I can ironman out this criticism, so let’s not dwell on it.</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile7760">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7763">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7763">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-02-22T23:32:00</p>
<div class="content"><div class="postrev" data-snap="1">Past!you is capable of predicting what future!you will do fairly well. There's not a whole lot future!you can do to hide your decision from present!you. Past!you could lie about your decision, but he will likely be caught in his lie. He could intentionally fail at predicting future!you's actions, but he will know it isn't an accurate prediction, and if he claims it is, he will likely be caught in his lie. Future!you's decision is entangled with past!you's prediction. Past!you's prediction is entangled with Paul Ekman's decision of whether or not to pick you up. As such, future!you's decision is entangled with Paul Ekman's decision.<br/><br/><blockquote class="uncited"><div>What does this have to do with my criticism of ‘intent’? Whatever we refer to by the word, I think we can agree that ‘I intend x’ isn’t logically equivalent to ‘X will happen’.</div></blockquote><br/><br/>No, but I think the thought experiment works better if you use "x will happen" instead of "I intend x to happen". If you want something closer to the normal meaning, perhaps "I predict that I will do x". That's still not how "intent" is normally defined. I'd say the original thought experiment is badly worded.<br/><br/><blockquote class="uncited"><div>What on earth makes you say it ‘usually leads to a worse outcome’?</div></blockquote><br/>Those who attempt your strategy are worse off than those who do not. Of those who actually get the chance to make the decision, those who attempt your strategy do better, but it's less likely for someone who attempts your strategy to make it to that point.<br/><br/>It occurs to me that this thread is named after EDT, but you're talking about an argument to distinguish EDT from TDT. I think I can argue better if we go with one to distinguish CDT from EDT, such as Newcomb's paradox.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7763">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7770">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7770">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2013-02-23T11:38:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>DanielLC wrote:</cite>Past!you <span style="font-weight: bold">is capable of predicting1</span> what future!you will do fairly well. There's not a whole lot future!you can do to hide <span style="font-weight: bold">your2</span> decision from present!you. <span style="font-weight: bold">Past3</span>!you could lie about your decision, but he will likely be caught in his lie. He could intentionally fail at predicting future!you's actions, but he will know it isn't an accurate prediction, and if he claims it is, he will likely be caught in his lie. Future!you's decision is entangled with past!you's <span style="font-weight: bold">prediction1</span>. <span style="font-weight: bold">Past3.1</span>!you's prediction is entangled with Paul Ekman's decision of whether or not to pick you up. <span style="font-weight: bold">As such, future!you's decision is entangled with Paul Ekman's decision.</span></div></blockquote><br/><br/>I think I agree much, maybe all of this in spirit until the last line, but at least one of us is misexplaining some details. The bits I've bolded and numbered are those where it doesn't hold for me, unless you replace them with something like the following:<br/><br/><span style="font-weight: bold">1</span> 'demonstrates/is evidence for' (via his track record of behaviour plus the fact that he shares so much personality makeup with future - and present - me)<br/><span style="font-weight: bold">2 </span>Just a semantic quibble I think, but for clarity, 'his'<br/><span style="font-weight: bold">3</span> Surely 'Present'? Esp in 3.1, we're not given any indication in the original thought experiment that he has any idea who we are. If you want to rewrite it so he does, I'm happy to consider that, but can we stick to one intuition-appeal at a time?<br/><br/>And the last line just doesn't seem to follow from anything. If A increases the likelihood of B it doesn't mean<br/><br/><blockquote class="uncited"><div>That's still not how "intent" is normally defined. I'd say the original thought experiment is badly worded.</div></blockquote><br/><br/>Can you rephrase it so that we're sure we're not arguing at cross purposes? My underlying claim is something like 'for any fully described variation on Parfit's hitchhiker, Newcomb's paradox or indeed any other such thought experiment, there is no utilitarian gain from thinking about it in terms of multiple possible 'decision theories' rather than expected utility", so to disprove the claim we need a well-described variant and a demonstration of how expected utility is inadequate.<br/><br/><blockquote class="uncited"><div>Those who attempt your strategy are worse off than those who do not. Of those who actually get the chance to make the decision, those who attempt your strategy do better, but it's less likely for someone who attempts your strategy to make it to that point.</div></blockquote><br/><br/>You keep asserting this, but I see no reason at all to believe it, or rather, no way of even parsing it. Can you clarify what you mean by a) 'My strategy', b) Any alternative and then explain to me how the alternative does better. I think 'my strategy' is - by definition - so general as to contain any possible best solutions. As such it is practically useless (or just not a strategy per se), and yet you seem to be saying there's a broader category into which it fits, which contains some other information without which I'm actually excluding plausible best solutions.<br/><br/>So many smart people with similar worldviews to mine make this claim that even though it seems conceptually incoherent I think I'm emotionally open to being persuaded of it, but it seems to entail answering some very basic questions that said smart people seem collectively unable to answer or even parse.<br/><br/><blockquote class="uncited"><div>It occurs to me that this thread is named after EDT, but you're talking about an argument to distinguish EDT from TDT. I think I can argue better if we go with one to distinguish CDT from EDT, such as Newcomb's paradox.</div></blockquote><br/><br/>I would prefer the focus to remain on DTs in general. I just don't see any way in which they aren't spurious metainformation.</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile7770">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7773">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7773">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-02-23T19:00:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote class="uncited"><div>Surely 'Present'?</div></blockquote><br/><br/>I don't like picking an arbitrary moment in the experiment to call the present, so I'm referring to the point further in the past as past, and the point further in the future as future.<br/><br/><blockquote class="uncited"><div>Can you rephrase it so that we're sure we're not arguing at cross purposes?</div></blockquote><br/><br/>That's probably something <a class="postlink" href="http://lesswrong.com/lw/2k/the_least_convenient_possible_world/">you should be doing yourself</a>, but sure.<br/><br/>Past!you is in the desert. Paul Ekman offers to give past!you a ride for $500. Past!you does not have money, but Paul says that he's willing to let future!you pay, so long as past!you thinks he will. However, he can tell if past!you is lying. He asks past!you if he has predicted future!you's actions to the best of his ability, and, if so, whether future!you will pay.<br/><br/>Past!you has access to your source code, for obvious reasons. Not even Past!you can run a perfect simulation of future!you, but he cannot be intentionally fooled. Given that you would make a certain decision under a certain circumstance, he has a 90% chance of correctly predicting it. If future!you would have an epiphany that changes his strategy of making decisions, past!you's simulation of future!you is 90% likely to have that epiphany and make that decision.<br/><br/><blockquote class="uncited"><div>You keep asserting this, but I see no reason at all to believe it, or rather, no way of even parsing it.</div></blockquote><br/><br/>People who would not pay die 90% of the time and save $500 the other 10%. People who would pay do better.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7773">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7803">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7803">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2013-02-26T22:58:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>DanielLC wrote:</cite>That's probably something <a class="postlink" href="http://lesswrong.com/lw/2k/the_least_convenient_possible_world/">you should be doing yourself</a>, but sure.</div></blockquote><br/><br/>If there is a version of the thought experiment I could imagine of which would show me to be wrong, then because it's a thought experiment I'd already know that I was wrong.<br/><br/><blockquote class="uncited"><div>Past!you is in the desert. Paul Ekman offers to give past!you a ride for $500. Past!you does not have money, but Paul says that he's willing to let future!you pay, so long as past!you thinks he will. However, he can tell if past!you is lying. He asks past!you if he has predicted future!you's actions to the best of his ability, and, if so, whether future!you will pay.<br/><br/>Past!you has access to your source code, for obvious reasons. Not even Past!you can run a perfect simulation of future!you, but he cannot be intentionally fooled. Given that you would make a certain decision under a certain circumstance, he has a 90% chance of correctly predicting it. If future!you would have an epiphany that changes his strategy of making decisions, past!you's simulation of future!you is 90% likely to have that epiphany and make that decision.</div></blockquote><br/><br/>Ok. Now tell me why your view ever wins over mine. I take it I am not allowed to assert that I would change future me's source code now, so that he believed me? If not, why would I have been able to do so in the past?</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile7803">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7806">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7806">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-02-27T05:47:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote class="uncited"><div>Now tell me why your view ever wins over mine.</div></blockquote><br/><br/>It's not actually my view. I lean more towards EDT. It is tempting though.<br/><br/>TDT wins because the TDT agent (usually) lives and the CDT and EDT agents (usually) die. What's the point of being rational if it just gets you killed?<br/><br/><blockquote class="uncited"><div>I take it I am not allowed to assert that I would change future me's source code now, so that he believed me?</div></blockquote><br/><br/>You can't precommit, if that's what you mean.<br/><br/><blockquote class="uncited"><div>If not, why would I have been able to do so in the past?</div></blockquote><br/><br/>I don't understand. When were you allowed to change your source code?<br/><br/>You can change your strategy. You can make whatever code you want future!you to follow and then start following it. You just can't force future!you to actually follow it. If you decide to be a TDT agent, you can be a TDT agent. However, if you want to be CDT and yet have future!you be TDT agent, then future!you will likely decide not to follow this strategy, and be a CDT agent and try to convince further future!you to change to TDT. To put it another way, one can't expect future!you to follow through on present!you's promises if present!you isn't willing to follow through on past!you's promises.<br/><br/>Also, you can modify the problem so that you simply don't have a chance to precommit. It does make it hard to make it realistic though (unless you accept the doomsday argument). Perhaps Omega finds you already passed out in the desert, scans your brain, and uses that to decide whether or not you'd pay.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7806">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7835">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7835">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2013-03-03T12:02:00</p>
<div class="content"><div class="postrev" data-snap="1"><blockquote><div><cite>DanielLC wrote:</cite>It's not actually my view. I lean more towards EDT. It is tempting though.<br/><br/>TDT wins because the TDT agent (usually) lives and the CDT and EDT agents (usually) die. What's the point of being rational if it just gets you killed?</div></blockquote><br/><br/>This is a description of winning means, not an explanation of why TDT will do it where the normal value-maximizing principle won't.<br/><br/><br/><blockquote class="uncited"><div>I don't understand. When were you allowed to change your source code?<br/><br/>You can change your strategy. You can make whatever code you want future!you to follow and then start following it. You just can't force future!you to actually follow it. If you decide to be a TDT agent, you can be a TDT agent. </div></blockquote><br/><br/>How is 'decide to be a TDT agent' not a subset of 'change my source code'? Presumably I wasn't a TDT agent before.</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile7835">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7838">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7838">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-03-04T06:48:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote class="uncited"><div>This is a description of winning means, not an explanation of why TDT will do it where the normal value-maximizing principle won't.</div></blockquote><br/><br/>A TDT agent is willing to pay, and will be rescued. A CDT or EDT agent is not, and will not.<br/><br/><blockquote class="uncited"><div>How is 'decide to be a TDT agent' not a subset of 'change my source code'?</div></blockquote><br/><br/>You are your source code. Deciding to be a TDT agent and having the source code of a TDT agent are the same thing. If you decide to be a TDT agent, there is no need to change your source code, because it already makes you a TDT agent.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7838">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7851">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7851">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2013-03-06T17:08:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote class="uncited"><div>A TDT agent is willing to pay, and will be rescued. A CDT or EDT agent is not, and will not.</div></blockquote><br/><br/>Why? What algorithm is he following? You can’t just assert that it’s one that’s functionally indistinguishable from others in almost any situation but that magically works here and expect me - or Paul Eckerman - to heed you.<br/><br/><blockquote class="uncited"><div>You are your source code. Deciding to be a TDT agent and having the source code of a TDT agent are the same thing. If you decide to be a TDT agent, there is no need to change your source code, because it already makes you a TDT agent.</div></blockquote><br/><br/>This is sophistry. Assuming I am a TDT, between birth and point A, I was something other than a TDT. Then I became a TDT. If I am my source code, that involved changing my source code. If that was allegedly possible then, why is it not allegedly possible now that Paul Eckerman is standing in front of me? You seem to just be defining this as suddenly having become impossible. I see no reason to accept your assertion that giving myself a ‘reboot with TDT software’ (whatever that is) instruction is impossible now but was possible two weeks ago.</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile7851">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7852">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7852">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-03-07T02:44:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote class="uncited"><div>Why?</div></blockquote><br/><br/>Because he's following an algorithm that results in paying.<br/><br/><blockquote class="uncited"><div>What algorithm is he following?</div></blockquote><br/><br/>Take the action such that, given that you would take this action in this situation, but not given anything else like the fact that you already saw Paul Eckerman pick you up, maximizes expected utility.<br/><br/><blockquote class="uncited"><div>If that was allegedly possible then, why is it not allegedly possible now that Paul Eckerman is standing in front of me?</div></blockquote><br/><br/>Like all humans, your source code is constant, but you don't have full control over it.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7852">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7855">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7855">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2013-03-07T12:25:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote class="uncited"><div>Take the action such that, given that you would take this action in this situation, but not given anything else like the fact that you already saw Paul Eckerman pick you up, maximizes expected utility.</div></blockquote><br/><br/>If your algorithm refers to anything about the situation at hand, then it’s clearly not applicable to decision-making in general. Maybe all I’m ultimately asking for is a description of TDT, but I want someone to show me specifically why it achieves something that ‘maximise expected utility’ doesn’t, and furthermore, something which is inconsistent with that maxim.<br/><br/>A thought I just had that might be behind our disagreement: <br/><br/>Would you claim that ‘maximise expected utility’ is actually two proposals rolled into one – one defining your overall goal in life G, one giving you an algorithm A to follow from moment to moment?<br/>If so, you might be claiming that TDT is consistent with (and a specific strategy for achieving G) but inconsistent  with and sometimes superior to A.<br/><br/>If so I think I would still probably disagree on both points – that G is not equivalent to A and that TDT is sometimes superior to A, but since the second disagreement follows from the first we could disregard the second and possibly gain greater focus.<br/><br/><blockquote class="uncited"><div> Like all humans, your source code is constant, but you don't have full control over it.</div></blockquote><br/><br/>This isn’t an answer. Why do I not have enough control to change it now, if I did N time ago?</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile7855">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p7857">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p7857">Re: Implications of Evidential Decision Theory</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2013-03-07T23:58:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote class="uncited"><div>If your algorithm refers to anything about the situation at hand, then it’s clearly not applicable to decision-making in general.</div></blockquote><br/><br/>I meant any given situation.<br/><br/>That being said, even if all I gave was a decision theory that referred to that situation in particular, it would suggest that it's part of a larger pattern.<br/><br/><blockquote class="uncited"><div>I want someone to show me specifically why it achieves something that ‘maximise expected utility’ doesn’t</div></blockquote><br/><br/>"Maximize expected utility" is not well-defined. I could tell you how it does better than CDT or EDT, but that's it.<br/><br/>EDT tells you not to pay Paul because the probability of survival given that he rescued you and you pay him is no higher than the probability of survival given that he rescued you and you do not. Because an EDT agent would make this decision, he dies.<br/><br/>TDT tells you to pay Paul, because if you're more likely to be alive given only that you pay Paul. Because a TDT agent would make this decision, he lives.<br/><br/><blockquote class="uncited"><div>Why do I not have enough control to change it now, if I did N time ago?</div></blockquote><br/><br/>You can decide to follow any decision theory. You cannot simply decide for your future self to fulfill promises you make now. This is not just an assumption the problem has. This is a fact about real life. If you want to know why, ask a psychologist.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile7857">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd> </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<input type="hidden" value="20130619002531/viewtopic.php?f=29&amp;t=839&amp;start=20"/><input type="hidden" value="20130619071727/viewtopic.php?f=29&amp;t=839"/>
                        <hr>
                        <div class="topic-actions">
                            <div class="pagination">
                                25 posts
                            </div>
                        </div>
                        <p><a href="./viewforum.php?f=10" class="left-box left" accesskey="r">Return to General discussion</a></p>
                    </div>
                    <div id="page-footer">
                    </div>
                </div>
                <div>
                    <a id="bottom" name="bottom" accesskey="z"></a>
                </div>
            </div>
            <div class="positioncorrection-bottom"></div>
        </div>
        <div class="bottom-left"></div>
        <div class="bottom-middle"></div>
        <div class="bottom-right"></div>
    </body>
</html>
