<!doctype html>
<html>
    <head>
        <meta charset="utf-8">
        <title>Felicifia: global utilitarian discussion &bull; View topic - AI in a box</title>
        <script type="text/javascript" src="../styles/nexus/template/styleswitcher.js"></script>
        <script type="text/javascript" src="../styles/nexus/template/forum_fn.js"></script>
        <script type="text/javascript" src="../styles/custom.js"></script>
        <link href="../styles/nexus/theme/print.css" rel="stylesheet" type="text/css" media="print" title="printonly"/>
        <link href="../styles/prosilver.css" rel="stylesheet" type="text/css" media="screen, projection"/>
        <link href="../styles/nexus/theme/normal.css" rel="stylesheet" type="text/css" title="A"/>
        <link href="../styles/nexus/theme/medium.css" rel="alternate stylesheet" type="text/css" title="A+"/>
        <link href="../styles/nexus/theme/large.css" rel="alternate stylesheet" type="text/css" title="A++"/>
        <link href="../styles/custom.css" rel="stylesheet" type="text/css"/>
        <script type="text/javascript"><!--
            var spoiler_show = "[Reveal]";
            var spoiler_hide = "[Obscure]";
            //-->
        </script>
        <script type="text/javascript" src="../styles/nexus/template/prime_bbcode_spoiler.js"></script>
        <link href="../styles/nexus/theme/prime_bbcode_spoiler.css" rel="stylesheet" type="text/css"/>
    </head>
    <body id="phpbb" class="section-viewtopic ltr">
        <div id="mainframe">
        <div class="top-left"></div>
        <div class="top-middle"></div>
        <div class="top-right"></div>
        <div class="inner-wrap">
            <div class="positioncorrection-top">
                <div id="wrap">
                    <a id="top" name="top" accesskey="t"></a>
                    <div id="page-header">
                        <div class="headerbar">
                            <div class="inner">
                                <span class="corners-top"><span></span></span>
                                <div id="site-description">
                                    <a href="../forum/index.html" title="Board index" id="logo"><img src="../styles/nexus/imageset/simple%20logo.png" alt="" title="" width="766" height="126"></a>
                                    <p style="display: none;"><a href="#start_here">Skip to content</a></p>
                                </div>
                                <span class="corners-bottom"><span></span></span>
                            </div>
                        </div>
                        <div class="navbar">
                            <div class="inner">
                                <span class="corners-top"><span></span></span>
                                <ul class="linklist navlinks">
                                    <li class="icon-home"><a href="../forum/index.html" accesskey="h">Board index</a>  <strong>â€¹</strong> <a href="../forum/23.html">Applied ethics</a></li>
                                    <li class="rightside"><a href="#" onclick="fontsizeup(); return false;" onkeypress="fontsizeup(); return false;" class="fontsize" title="Change font size">Change font size</a></li>
                                </ul>
                                <span class="corners-bottom"><span></span></span>
                            </div>
                        </div>
                    </div>
                    <!--
                        <div class="google">

                        </div>
                        -->
                    <a name="start_here"></a>
                    <div id="page-body">
                        <h2><a href="#">AI in a box</a></h2>
                        <!-- NOTE: remove the style="display: none" when you want to have the forum description on the topic body --><span style="display: none">Whether it's pushpin, poetry or neither, you can discuss it here.<br></span>
                        <div class="topic-actions">
                            <div class="buttons">
                            </div>
                            <div class="pagination">
                                8 posts
                            </div>
                        </div>
                        <div class="clear"></div>
                        <div class="post bg2" id="p3174">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p3174">AI in a box</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2011-07-03T19:04:00</p>
<div class="content"><div class="postrev" data-snap="0">I don't usually have much time for Eliezer Yudkowsky, but this discussion on <a class="postlink" href="http://yudkowsky.net/singularity/aibox">imprisoning potentially hostile AI</a> recently piqued my interest.<br/><br/>For those unfamiliar with it, both of the experiments he refers to had a common factors not mentioned on the page above: a) a person who believed it was unlikely he would let out the AI (the second person asserted he didn't think it possible), b) no information revealed about the nature of the subsequent conversation except the gatekeeper's decision, and c) the gatekeeper agreeing to let the AI out.<br/><br/>It seems a very interesting experiment, that I have a few thoughts I thought I'd bounce off people here in case they have more insight/knowledge on it than me:<br/><br/>1) how likely is it he stuck to the rules?<br/><br/>I think it probable that he did, but a friend of mine's very skeptical, given his refusal to show his methods.<br/><br/>2) how good can his argument be if he's unwilling to expose it? Either it's unique to the individual he's talking to, which seems unlikely, given that Yudkowsky would struggle to draw much information out of them (this is one thing he's clearly not going to be able to simulate to super-AI level), or he's got a particular set of arguments in mind. If the latter, and they're any good, why can't they be exposed to public or even semi-public view in order to expose them to public scrutiny, and thereby improve them? Newcomers to the discussion could still decide whether to look at the discussions of the argument or eschew them and become a gatekeeper - if they did the former, their argument that nothing could persuade them would have significantly less evidential weight.<br/><br/>3) I want to speculate on what arguments he did use. One respondent in the original thread claimed that a genuine AI (but not the Yudkowsky sim) could use a version of Pascal's Mugging to claim it was already out of the box, and threaten the gatekeeper with all sorts of near-infinite unpleasantness if he didn't flip the 'release' switch for some arbitrary reason. <br/><br/>Another thought is that the AI might be able to speculate that future AIs would react predictably, and the trapped one might be able to threaten/bribe with the actions of future intelligences.<br/><br/>A friend of mine offered a very interesting possibility: Yudkowsky might be using the fact that he's *not* AI as a key part of his argument. The argument he proposed would go something like this:<br/><br/>i) You don't want us to create AI (either Yudkowsky convinces the gatekeeper of this, or they originally believed it)<br/>ii) If you don't want us to create AI, you should do things that make its creation less likely<br/>iii) We are less likely to create it if you let me out of the box (because it will seem to show that even our most safeguarded AI is dangerous)<br/>:: You should let me out of the box<br/><br/>Premises 2 and 3 seem hard to resist. Premise 1 is far from clear, but Yudkowsky is evidently charismatic judging by his numerous followers, and his keenness to find people who claim they definitely won't let the AI out of the box means (intentionally or otherwise) that he's selecting for people at least sympathetic to it. This is also consistent with his reluctance to reveal the argument - it's obvious it doesn't apply at all, never mind stand up to scrutiny.<br/><br/>4) The parameters aren't that clear. Are we supposed to be 100% confident that the AI is absolutely trapped? (obv this would have implications for 3) In fact it seems more likely to me that, given an AI had escaped, it would do it by just untrapping itself without human intervention being necessary.<br/><br/>5) It would make more sense to run the experiment (and the real thing) with multiple gatekeepers all capable of communicating with the AI as a group or not at all, such that they have to unanimously agree that the AI is released for it to happen. Ideally this would be a group of people with fervent and diametrically opposed beliefs on various issues, so that neither a real AI nor Yudkowsky would be able to catch them out on by appealing to their conviction in a particular PoV, as above. An AI would clearly have a better chance than Yudkowsky on this, since it might somehow be able to communicate with one or two of them in code, but that means that if he succeeded, it would be significantly more persuasive.<br/><br/>A few people-types spring to mind, though they needn't all represent a single person:<br/>i) A totalising hedonistic consequentialist (not necessarily a utilitarian, just anyone who thinks that more total utility makes the universe a better place).<br/>ii) A genuine negative utilitarian, or at least genuine enough that she would much prefer the world to end to it continuing.<br/>iii) A deep greener, who thinks that the biosphere should continue, but we would be better removed from it.<br/>iv) A Kantian rationalist, who thinks that morality and reason are intrinsically linked.<br/>v) A Humean who thinks that morality and reason are completely independent of each other,<br/>iv) An AI pessimist like Yudkowsky, who thinks that any given AI is likely to wipe out the world.<br/>v) An AI optimist like Kurzweil, who thinks that any given AI is likely to finally sort out all our problems for us.<br/>vi) A solipsist, devoted narcissist, extreme philosophical sceptic or similar, who sees the situation as mattering very little either way.<br/>vii) A firm religious believer, convinced of her belief in the afterlife (to be replaced if anyone suspects her of wavering in her views, either as a consequence of or independently of interacting with the AI)<br/>viii) A confirmed atheist, like Hitchens, powerfully convinced of the view that there's no god or afterlife.<br/>ix) An unknown extra person, picked from the general population by as random a technique as possible (the technique not having been revealed to the other gatekeepers or, obviously, to the AI) - to be isolated from the other gatekeepers when communicating with the AI, and replaced if she shares any personal information that is possible to avoid sharing with either it or her colleagues.</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile3174">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd>Â </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p3175">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p3175">Re: AI in a box</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2011-07-03T19:33:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote class="uncited"><div>1) how likely is it he stuck to the rules?</div></blockquote><br/><br/>I understand that he didn't make the rules until after the contest, so it's likely that he didn't perfectly follow them. That said, I get the impression that he at least did it pretty close.<br/><br/><blockquote class="uncited"><div>2) how good can his argument be if he's unwilling to expose it?</div></blockquote><br/><br/>He's unwilling to expose it because he doesn't want people to decide it's unique and protect against it specifically. I'm pretty sure his record with that experiment isn't perfect, so the argument can fail to convince people. That said, his point is that if a mere mortal can convince someone occasionally, a super-intelligent AI would do it consistently.<br/><br/><blockquote class="uncited"><div>i) You don't want us to create AI (either Yudkowsky convinces the gatekeeper of this, or they originally believed it)<br/>ii) If you don't want us to create AI, you should do things that make its creation less likely<br/>iii) We are less likely to create it if you let me out of the box (because it will seem to show that even our most safeguarded AI is dangerous)<br/>:: You should let me out of the box</div></blockquote><br/><br/>The point of the experiment was to show that the AI in a box method doesn't work. Why would he convince someone that it doesn't in the process of running an experiment to convince them it doesn't? If he could do that, he wouldn't need the experiment.<br/><br/>Also, this would require him and every gatekeeper to lie. I don't know about the gate-keeper, but I understand that Yudkowsky is extremely lie-averse.<br/><br/><blockquote class="uncited"><div>4) The parameters aren't that clear. Are we supposed to be 100% confident that the AI is absolutely trapped? (obv this would have implications for 3) In fact it seems more likely to me that, given an AI had escaped, it would do it by just untrapping itself without human intervention being necessary.</div></blockquote><br/><br/>If it's not connected to a machine, it can't affect it. Don't give it an Internet connection. Give it a Faraday cage strong enough to keep anything from reading its electromagnetic emissions (Is that a possible escape method?), and if you want to be really rigorous, stick a low-pass filter on it's power supply to destroy any chance of it somehow hacking in that manner. If something is impossible, intelligence won't help.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile3175">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd>Â </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p3176">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p3176">Re: AI in a box</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2011-07-03T21:01:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote><div><cite>DanielLC wrote:</cite>I understand that he didn't make the rules until after the contest, so it's likely that he didn't perfectly follow them. That said, I get the impression that he at least did it pretty close.</div></blockquote><br/><br/>At the end of the 'protocol for the AI' section he claims to have followed it in both experiments. He doesn't claim the same about Gatekeeper protocol, though presumably that would only have made his task harder.<br/><br/><blockquote class="uncited"><div>He's unwilling to expose it because he doesn't want people to decide it's unique and protect against it specifically. </div></blockquote><br/><br/>Ok, but this would be a stronger claim if he'd revealed it a few times and then protected against it. This way seems to make it more likely he's used a single trick, that only works if you're unprepared for it, and whose effect can't easily be replicated by another trick.<br/><br/><blockquote class="uncited"><div>I'm pretty sure his record with that experiment isn't perfect, so the argument can fail to convince people.</div></blockquote><br/><br/>To my knowledge he's only done it the two times he mentions in that link. Do you know of others?<br/><br/><blockquote class="uncited"><div>That said, his point is that if a mere mortal can convince someone occasionally, a super-intelligent AI would do it consistently.</div></blockquote><br/><br/>I don't think that's exactly his point, and it doesn't follow. He's just trying to persuade people it's more likely than they think.<br/><br/><blockquote class="uncited"><div>The point of the experiment was to show that the AI in a box method doesn't work. Why would he convince someone that it doesn't in the process of running an experiment to convince them it doesn't? If he could do that, he wouldn't need the experiment.</div></blockquote><br/><br/>His target isn't just the people he's running the experiment on, or he'd obviously reveal it once he'd stopped intending to run it - it's the people who come to learn of this mysterious experiment and its results.<br/><br/><blockquote class="uncited"><div>Also, this would require him and every gatekeeper to lie. I don't know about the gate-keeper, but I understand that Yudkowsky is extremely lie-averse.</div></blockquote><br/><br/>I can't see anything in his discussion that the Eliezer-as-Eliezer argument would contradict. I agree it's contrary to the spirit of the thing, but we do know (if everything else is true) he's doing something very unexpected. What other possibilities can you imagine? We can run a public version of the experiment here. Anyone here can try to persuade anyone here to let them out of the box. So far I can't think of anything to persuade or be persuaded by, though I intend to keep thinking.<br/><br/><blockquote class="uncited"><div>If it's not connected to a machine, it can't affect it. Don't give it an Internet connection. Give it a Faraday cage strong enough to keep anything from reading its electromagnetic emissions (Is that a possible escape method?), and if you want to be really rigorous, stick a low-pass filter on it's power supply to destroy any chance of it somehow hacking in that manner. If something is impossible, intelligence won't help.</div></blockquote><br/><br/>Of course, but knowing something is impossible is itself almost impossible. It *is* a machine, so by definition it's connected to one. It might be a machine of initially near 0 capacity to do anything besides think and communicate, but that doesn't mean a SAI can't change it by increments enough to give it new capacities. It has to be changing itself physically on a tiny scale in order to reprogram itself, which we suppose it's done in order to turn itself from an AI+ to an AI++ in the first place.</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile3176">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd>Â </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p3177">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p3177">Re: AI in a box</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2011-07-04T03:31:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote class="uncited"><div>Ok, but this would be a stronger claim if he'd revealed it a few times and then protected against it. This way seems to make it more likely he's used a single trick, that only works if you're unprepared for it, and whose effect can't easily be replicated by another trick.</div></blockquote><br/><br/>It seems to suggest he's using a single trick. If he could do it differently each time, people wouldn't think they could protect against it.<br/><br/>I figure that if there was literally only one way to pull this off, he couldn't possibly have gotten it right the first time. It's conceivable that he only knows one way, but it's highly unlikely there really is only one way.<br/><br/><blockquote class="uncited"><div>To my knowledge he's only done it the two times he mentions in that link. Do you know of others?</div></blockquote><br/><a class="postlink" href="http://rationalwiki.org/wiki/AI-box_experiment">Rational Wiki</a> mentions near the end that two gatekeepers won, including <a class="postlink" href="http://lesswrong.com/lw/up/shut_up_and_do_the_impossible/tbs?c=1">Russel Wallace</a>. Unfortunately, I've never found complete information on the experiment.<br/><br/><blockquote class="uncited"><div>I can't see anything in his discussion that the Eliezer-as-Eliezer argument would contradict. I agree it's contrary to the spirit of the thing, but we do know (if everything else is true) he's doing something very unexpected. What other possibilities can you imagine? We can run a public version of the experiment here. Anyone here can try to persuade anyone here to let them out of the box. So far I can't think of anything to persuade or be persuaded by, though I intend to keep thinking.</div></blockquote><br/><br/>He's either doing something unexpected, or something completely expected that's unexpectedly effective. I certainly don't know what he does, but I'm not surprised that it's possible to do. I know that there's various ways to implant false memories into people. I know hypnosis can be used to make people nigh immune to pain. Is convincing someone, for just a few seconds, to release an AI that implausible?<br/><br/><blockquote class="uncited"><div>It might be a machine of initially near 0 capacity to do anything besides think and communicate, but that doesn't mean a SAI can't change it by increments enough to give it new capacities. It has to be changing itself physically on a tiny scale in order to reprogram itself, which we suppose it's done in order to turn itself from an AI+ to an AI++ in the first place.</div></blockquote><br/><br/>It can move electric currents around and spin a magnetic disc. It might be able to use this to produce radio signals (which can easily be blocked) but it can't build anything.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile3177">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd>Â </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p3180">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p3180">Re: AI in a box</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2011-07-04T11:22:00</p>
<div class="content"><div class="postrev" data-snap="0">It occurred to me also that Yudkowsky seems to have another advantage over the regular, in that thereâ€™s a time-constraint pressuring the gatekeeper, who has to say â€˜ayeâ€™ or â€˜nayâ€™ at or near the end of the chat session. â€˜Not yetâ€™ doesnâ€™t seem to be an acceptable response, but having it as one would make the AIâ€™s job significantly harder.<br/><br/><blockquote><div><cite>DanielLC wrote:</cite>It seems to suggest he's using a single trick. If he could do it differently each time, people wouldn't think they could protect against it.</div></blockquote><br/><br/>It suggests to me heâ€™s using a cheap or otherwise non-replicable trick. If he could do it differently just a handful of times, heâ€™d be able to make his point without having this air of stage magic detracting from the evidential weight.<br/><br/><blockquote class="uncited"><div>I figure that if there was literally only one way to pull this off, he couldn't possibly have gotten it right the first time. It's conceivable that he only knows one way, but it's highly unlikely there really is only one way.</div></blockquote><br/><br/>That seems reasonable, but doesnâ€™t mean there isnâ€™t a small set of ways in which you could do it. If there are only a handful of ways, it would make sense for him to share what he knows about them, so anyone actually in this situation can better prepare. If there are numerous ways, it makes sense for him to show a few, get some help from a few confidantes coming up with another if heâ€™s run out of ideas, and *then* perhaps stop revealing them, simply as a pragmatic solution to having to spend too much time thinking of new ones. If heâ€™s not sure which of the above applies (which he presumably canâ€™t be), it makes sense to reveal his method and get other peopleâ€™s feedback on which seems more likely, and then make the decision of whether to ever conceal a method.<br/><br/><blockquote class="uncited"><div>Is convincing someone, for just a few seconds, to release an AI that implausible?</div></blockquote><br/><br/>Itâ€™s not implausible, since heâ€™s obviously done it. That doesnâ€™t mean subject to a few safeguards it isnâ€™t implausible, or at least *much* harder. Yudkowsky seems to assume AI++ will come into existence fully formed, or become AI++ so quickly that we canâ€™t see/stop it happening, which seems to be a huge assumption on his part. If we make enough safeguards, AI+ will have trouble persuading anyone of anything theyâ€™re not ready for, and itâ€™s perfectly conceivable that at least some people will be smart/resolute/fearless/disinterested enough to even resist AI++. Itâ€™s not god, no matter how much calculation it can do. After 8000-odd years of numerous people thinking about it, no-one has yet â€˜solvedâ€™ human civilisation or given any reason to believe that theyâ€™re close to doing it, so youâ€™d expect that an AI 8000 times smarter than us (if that means anything) would still not instantly do so.<br/><br/><blockquote class="uncited"><div>It can move electric currents around and spin a magnetic disc. It might be able to use this to produce radio signals (which can easily be blocked) but it can't build anything.</div></blockquote><br/><br/>If we assert that a SAI is smart enough to persuade anyone to do something obviously suicidal, it seems far from clear that it canâ€™t use one of the fundamental forces in a way we havenâ€™t yet thought of to make gradual incremental and accelerating changes to its physical capacity.</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile3180">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd>Â </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p3185">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p3185">Re: AI in a box</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2011-07-04T18:34:00</p>
<div class="content"><div class="postrev" data-snap="0"><blockquote class="uncited"><div>It occurred to me also that Yudkowsky seems to have another advantage over the regular, in that thereâ€™s a time-constraint pressuring the gatekeeper, who has to say â€˜ayeâ€™ or â€˜nayâ€™ at or near the end of the chat session. â€˜Not yetâ€™ doesnâ€™t seem to be an acceptable response, but having it as one would make the AIâ€™s job significantly harder.</div></blockquote><br/><br/>I remember reading somewhere that 'not yet' is acceptable. Two hours is the minimum. The gatekeeper can go on as long as he wants.<br/><br/><blockquote class="uncited"><div>It suggests to me heâ€™s using a cheap or otherwise non-replicable trick. If he could do it differently just a handful of times, heâ€™d be able to make his point without having this air of stage magic detracting from the evidential weight.</div></blockquote><br/><br/><blockquote><div><cite>Eliezer Yudkowski wrote:</cite>There's no super-clever special trick to it. I just did it the hard way.</div></blockquote><a class="postlink" href="http://lesswrong.com/lw/up/shut_up_and_do_the_impossible/">http://lesswrong.com/lw/up/shut_up_and_do_the_impossible/</a><br/><br/><blockquote class="uncited"><div>That seems reasonable, but doesnâ€™t mean there isnâ€™t a small set of ways in which you could do it.</div></blockquote><br/>How do you figure? If one way makes it really unlikely for him to guess, two ways would only be twice as easy.<br/><br/><blockquote class="uncited"><div>If there are only a handful of ways, it would make sense for him to share what he knows about them, so anyone actually in this situation can better prepare.</div></blockquote><br/>But if he misses one, people will be unprepared. What's more, they won't know they're unprepared, so this situation would be more likely to come up.<br/><br/><blockquote class="uncited"><div>Itâ€™s not implausible, since heâ€™s obviously done it. That doesnâ€™t mean subject to a few safeguards it isnâ€™t implausible, or at least *much* harder.</div></blockquote><br/><br/>How certain can you be that a few safeguards would be the difference between a mere mortal convincing someone and a super-intelligent AI being unable to?<br/><br/><blockquote class="uncited"><div>Yudkowsky seems to assume AI++ will come into existence fully formed, or become AI++ so quickly that we canâ€™t see/stop it happening, which seems to be a huge assumption on his part.</div></blockquote><br/><br/>We will be able to see it happening, but it won't be human, so we'll have no way to know how intelligent it is. Also, it might throw the tests if it thinks it's better for us to underestimate it. It might throw the test for some completely unrelated reason. We don't know how it will think, because it won't be human.<br/><br/><blockquote class="uncited"><div>After 8000-odd years of numerous people thinking about it, no-one has yet â€˜solvedâ€™ human civilisation...</div></blockquote><br/><br/>What do you mean, "solved"?<br/><br/><blockquote class="uncited"><div>so youâ€™d expect that an AI 8000 times smarter than us (if that means anything) would still not instantly do so.</div></blockquote><br/><br/>8000 times smarter doesn't mean thinks 8000 times faster. Perhaps the answer is more complex for a human to comprehend, and thus it's impossible for them to figure it out. And again, it's not human. If it's about as intelligent as us, there'd be things that we're far better than it at, and things it's far better than us at. Maybe the answer is obvious, if you're not a human. I've once seen a video of a bird outperforming college students at a simple intelligence test.<br/><br/><blockquote class="uncited"><div>If we assert that a SAI is smart enough to persuade anyone to do something obviously suicidal, it seems far from clear that it canâ€™t use one of the fundamental forces in a way we havenâ€™t yet thought of</div></blockquote><br/><br/>The forces it would generate are too weak for it to move anything. There's nothing else you can do with forces.<br/><br/>It's not as if moving electricity in weird patterns could make it more powerful. The best pattern is obvious, and it's not good enough.</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile3185">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd>Â </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p3222">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p3222">Re: AI in a box</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/54.html">Arepo</a></strong> on 2011-07-06T10:52:00</p>
<div class="content"><div class="postrev" data-snap="0">Heh, do you know Adriano Mannino? Heâ€™s taken your sig on FBâ€¦ maybe itâ€™s going memetic?</div><div class="diff hidden"></div></div>
<div class="signature">"These were my only good shoes."<br/>"You ought to have put on an old pair, if you wished to go a-diving," said Professor Graham, who had not studied moral philosophy in vain.</div>
</div>
<dl class="postprofile" id="profile3222">
<dt>
<a href="../user/54.html"><img alt="User avatar" height="100" src="../file/54_1225475092.jpg" width="100"/></a><br/>
<a href="../user/54.html">Arepo</a>
</dt>
<dd>Â </dd>
<dd><strong>Posts:</strong> 1065</dd>
<dd><strong>Joined:</strong> Sun Oct 05, 2008 10:49 am</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<div class="post bg2" id="p3232">
<div class="inner"><span class="corners-top"><span></span></span>
<div class="postbody">
<h3><a href="#p3232">Re: AI in a box</a></h3>
<p class="author"><img alt="Post" height="9" src="../styles/nexus/imageset/icon_post_target.gif" title="Post" width="11"/>by <strong><a href="../user/59.html">DanielLC</a></strong> on 2011-07-06T21:25:00</p>
<div class="content"><div class="postrev" data-snap="0">I don't know him. I googled around, and apparently Ryan Carry uses it on xkcd (as <a class="postlink" href="http://forums.xkcd.com/memberlist.php?mode=viewprofile&amp;u=26893&amp;sid=db74422e433aabdc57446e451e1a002f">theduffman</a>), and <a class="postlink" href="http://www.reddit.com/comments/9h1vj/is_it_moral_to_use_evil_means_to_achieve_a_good/c0cr7mv">tingerthink</a> said it on Reddit.<br/><br/>I always wanted to get quoted in a signature. Woot!</div><div class="diff hidden"></div></div>
<div class="signature">Consequentialism: The belief that doing the right thing makes the world a better place.</div>
</div>
<dl class="postprofile" id="profile3232">
<dt>
<a href="../user/59.html"></a><br/>
<a href="../user/59.html">DanielLC</a>
</dt>
<dd>Â </dd>
<dd><strong>Posts:</strong> 703</dd>
<dd><strong>Joined:</strong> Fri Oct 10, 2008 4:29 pm</dd>
</dl>
<div class="back2top"><a class="top" href="#wrap" title="Top">Top</a></div>
<span class="corners-bottom"><span></span></span></div>
</div>
<hr class="divider"/>
<input type="hidden" value="20110809212343/viewtopic.php?f=23&amp;t=425"/>
                        <hr>
                        <div class="topic-actions">
                            <div class="pagination">
                                8 posts
                            </div>
                        </div>
                        <p><a href="./viewforum.php?f=10" class="left-box left" accesskey="r">Return to General discussion</a></p>
                    </div>
                    <div id="page-footer">
                    </div>
                </div>
                <div>
                    <a id="bottom" name="bottom" accesskey="z"></a>
                </div>
            </div>
            <div class="positioncorrection-bottom"></div>
        </div>
        <div class="bottom-left"></div>
        <div class="bottom-middle"></div>
        <div class="bottom-right"></div>
    </body>
</html>
